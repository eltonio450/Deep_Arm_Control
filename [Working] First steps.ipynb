{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of importation\n"
     ]
    }
   ],
   "source": [
    "# Derived from keras-rl\n",
    "import opensim as osim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, merge\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "from osim.env import *\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "print(\"End of importation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments...\n",
      "Namespace(model='example.h5f', steps=10000, visualize=False)\n",
      "Environment generated\n"
     ]
    }
   ],
   "source": [
    "print(\"Arguments...\")\n",
    "#print(args)\n",
    "\n",
    "args = argparse.Namespace(model='example.h5f', steps=10000, visualize=False)\n",
    "print(args)\n",
    "# Load walking environment\n",
    "env = ArmEnv(args.visualize)\n",
    "print(\"Environment generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_1 (Flatten)              (None, 14)            0           flatten_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 32)            480         flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            1056        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 32)            1056        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 6)             198         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 6)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2790\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 14)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 14)            0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 20)            0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 64)            1344        merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 64)            0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 64)            4160        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 64)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 64)            4160        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 64)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             65          activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 1)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 9729\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Total number of steps in training\n",
    "nallsteps = args.steps\n",
    "\n",
    "# Create networks for DDPG\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('sigmoid'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = merge([action_input, flattened_observation], mode='concat')\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(input=[action_input, observation_input], output=x)\n",
    "print(critic.summary())\n",
    "\n",
    "# Set up the agent for training\n",
    "#warning: a too high limit gives errors\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.2, size=env.noutput)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  delta_clip=1.)\n",
    "# agent = ContinuousDQNAgent(nb_actions=env.noutput, V_model=V_model, L_model=L_model, mu_model=mu_model,\n",
    "#                            memory=memory, nb_steps_warmup=1000, random_process=random_process,\n",
    "#                            gamma=.99, target_model_update=0.1)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(action_input)\n",
    "print(observation_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "   500/100000: episode: 1, duration: 10.582s, episode steps: 500, steps per second: 47, episode reward: -579.349, mean reward: -1.159 [-3.245, -0.133], mean action: 0.322 [-0.645, 1.043], mean observation: -0.531 [-1000.000, 1000.000], loss: 0.959434, mean_absolute_error: 1.119451, mean_q: -1.190162\n",
      "  1000/100000: episode: 2, duration: 10.345s, episode steps: 500, steps per second: 48, episode reward: -249.679, mean reward: -0.499 [-1.036, -0.179], mean action: 0.374 [-0.433, 1.273], mean observation: -0.498 [-923.802, 897.097], loss: 0.744982, mean_absolute_error: 0.900031, mean_q: -1.652770\n",
      "  1500/100000: episode: 3, duration: 9.116s, episode steps: 500, steps per second: 55, episode reward: -137.106, mean reward: -0.274 [-1.496, -0.020], mean action: 0.506 [-0.413, 1.420], mean observation: -0.137 [-346.360, 322.495], loss: 0.301973, mean_absolute_error: 0.409812, mean_q: -1.380452\n",
      "  2000/100000: episode: 4, duration: 12.381s, episode steps: 500, steps per second: 40, episode reward: -602.093, mean reward: -1.204 [-2.208, -0.202], mean action: 0.380 [-0.420, 1.599], mean observation: 0.247 [-1000.000, 1000.000], loss: 0.252827, mean_absolute_error: 0.373233, mean_q: -1.622769\n",
      "  2500/100000: episode: 5, duration: 20.846s, episode steps: 500, steps per second: 24, episode reward: -1343.914, mean reward: -2.688 [-4.301, -0.046], mean action: 0.258 [-0.504, 1.404], mean observation: -0.461 [-1000.000, 1000.000], loss: 0.198834, mean_absolute_error: 0.336603, mean_q: -2.238573\n",
      "  3000/100000: episode: 6, duration: 7.708s, episode steps: 500, steps per second: 65, episode reward: -739.454, mean reward: -1.479 [-2.432, -0.163], mean action: 0.488 [-0.469, 1.426], mean observation: -0.346 [-1000.000, 1000.000], loss: 0.123834, mean_absolute_error: 0.256737, mean_q: -2.970305\n",
      "  3500/100000: episode: 7, duration: 8.035s, episode steps: 500, steps per second: 62, episode reward: -335.071, mean reward: -0.670 [-2.043, -0.464], mean action: 0.561 [-0.748, 1.473], mean observation: -0.088 [-399.875, 834.090], loss: 0.115026, mean_absolute_error: 0.238668, mean_q: -3.342197\n",
      "  4000/100000: episode: 8, duration: 15.019s, episode steps: 500, steps per second: 33, episode reward: -389.448, mean reward: -0.779 [-2.121, -0.057], mean action: 0.226 [-0.905, 1.568], mean observation: -0.103 [-12.987, 11.222], loss: 0.107435, mean_absolute_error: 0.227755, mean_q: -3.502575\n",
      "  4500/100000: episode: 9, duration: 7.249s, episode steps: 500, steps per second: 69, episode reward: -1444.321, mean reward: -2.889 [-3.624, -0.039], mean action: 0.442 [-0.628, 1.654], mean observation: 0.127 [-435.950, 466.858], loss: 0.091935, mean_absolute_error: 0.211401, mean_q: -4.025254\n",
      "  5000/100000: episode: 10, duration: 19.359s, episode steps: 500, steps per second: 26, episode reward: -990.035, mean reward: -1.980 [-3.573, -0.162], mean action: 0.085 [-0.759, 1.140], mean observation: 0.032 [-1000.000, 568.281], loss: 0.100254, mean_absolute_error: 0.226486, mean_q: -4.732561\n",
      "  5500/100000: episode: 11, duration: 26.607s, episode steps: 500, steps per second: 19, episode reward: -1475.339, mean reward: -2.951 [-3.869, -0.034], mean action: 0.336 [-0.383, 1.166], mean observation: -0.107 [-1000.000, 1000.000], loss: 0.205869, mean_absolute_error: 0.355582, mean_q: -5.503153\n",
      "  6000/100000: episode: 12, duration: 13.225s, episode steps: 500, steps per second: 38, episode reward: -717.093, mean reward: -1.434 [-2.568, -0.511], mean action: 0.365 [-0.417, 1.430], mean observation: 0.148 [-399.251, 1000.000], loss: 0.228970, mean_absolute_error: 0.380074, mean_q: -6.251937\n",
      "  6500/100000: episode: 13, duration: 12.640s, episode steps: 500, steps per second: 40, episode reward: -456.988, mean reward: -0.914 [-1.703, -0.071], mean action: 0.075 [-0.553, 1.863], mean observation: -0.116 [-1000.000, 816.927], loss: 0.195614, mean_absolute_error: 0.342984, mean_q: -6.510628\n",
      "  7000/100000: episode: 14, duration: 9.363s, episode steps: 500, steps per second: 53, episode reward: -572.181, mean reward: -1.144 [-3.233, -0.075], mean action: 0.404 [-0.424, 1.475], mean observation: -0.145 [-13.250, 253.042], loss: 0.211880, mean_absolute_error: 0.357882, mean_q: -6.930766\n",
      "  7500/100000: episode: 15, duration: 8.471s, episode steps: 500, steps per second: 59, episode reward: -118.185, mean reward: -0.236 [-1.208, -0.011], mean action: 0.430 [-0.992, 1.495], mean observation: -0.157 [-13.353, 8.052], loss: 0.200473, mean_absolute_error: 0.348217, mean_q: -7.206765\n",
      "  8000/100000: episode: 16, duration: 7.665s, episode steps: 500, steps per second: 65, episode reward: -203.564, mean reward: -0.407 [-1.695, -0.261], mean action: 0.472 [-0.253, 1.268], mean observation: -0.127 [-35.710, 275.415], loss: 0.194301, mean_absolute_error: 0.340929, mean_q: -7.282936\n",
      "  8500/100000: episode: 17, duration: 7.713s, episode steps: 500, steps per second: 65, episode reward: -172.920, mean reward: -0.346 [-1.509, -0.041], mean action: 0.353 [-0.466, 1.503], mean observation: -0.024 [-1000.000, 1000.000], loss: 0.172410, mean_absolute_error: 0.321485, mean_q: -7.430061\n",
      "  9000/100000: episode: 18, duration: 6.774s, episode steps: 500, steps per second: 74, episode reward: -469.293, mean reward: -0.939 [-1.375, -0.088], mean action: 0.651 [-0.124, 1.458], mean observation: -0.044 [-13.101, 449.148], loss: 0.159642, mean_absolute_error: 0.307823, mean_q: -7.572115\n",
      "  9500/100000: episode: 19, duration: 8.817s, episode steps: 500, steps per second: 57, episode reward: -151.042, mean reward: -0.302 [-1.282, -0.075], mean action: 0.550 [-0.621, 1.989], mean observation: -0.156 [-43.047, 216.905], loss: 0.224239, mean_absolute_error: 0.374842, mean_q: -7.792754\n",
      " 10000/100000: episode: 20, duration: 10.353s, episode steps: 500, steps per second: 48, episode reward: -353.144, mean reward: -0.706 [-2.643, -0.170], mean action: 0.448 [-0.557, 1.466], mean observation: 0.155 [-1000.000, 1000.000], loss: 0.171079, mean_absolute_error: 0.318620, mean_q: -7.935836\n",
      " 10500/100000: episode: 21, duration: 20.211s, episode steps: 500, steps per second: 25, episode reward: -1178.348, mean reward: -2.357 [-3.892, -0.115], mean action: 0.582 [-0.356, 1.803], mean observation: -1.284 [-1000.000, 1000.000], loss: 0.303922, mean_absolute_error: 0.462123, mean_q: -8.312589\n",
      " 11000/100000: episode: 22, duration: 10.686s, episode steps: 500, steps per second: 47, episode reward: -424.520, mean reward: -0.849 [-1.518, -0.145], mean action: 0.501 [-0.410, 1.516], mean observation: -0.624 [-1000.000, 1000.000], loss: 0.530499, mean_absolute_error: 0.714444, mean_q: -8.854510\n",
      " 11500/100000: episode: 23, duration: 8.453s, episode steps: 500, steps per second: 59, episode reward: -251.068, mean reward: -0.502 [-1.590, -0.007], mean action: 0.585 [-0.473, 1.400], mean observation: -0.161 [-112.197, 41.926], loss: 0.490295, mean_absolute_error: 0.681938, mean_q: -8.762753\n",
      " 12000/100000: episode: 24, duration: 9.157s, episode steps: 500, steps per second: 55, episode reward: -371.908, mean reward: -0.744 [-2.203, -0.052], mean action: 0.720 [-0.189, 1.451], mean observation: 0.040 [-1000.000, 1000.000], loss: 0.423163, mean_absolute_error: 0.615106, mean_q: -9.012693\n",
      " 12500/100000: episode: 25, duration: 9.881s, episode steps: 500, steps per second: 51, episode reward: -663.831, mean reward: -1.328 [-2.395, -0.063], mean action: 0.837 [-0.240, 1.679], mean observation: -0.120 [-1000.000, 1000.000], loss: 0.344589, mean_absolute_error: 0.536277, mean_q: -9.151932\n",
      " 13000/100000: episode: 26, duration: 8.390s, episode steps: 500, steps per second: 60, episode reward: -284.124, mean reward: -0.568 [-1.227, -0.023], mean action: 0.218 [-0.828, 1.048], mean observation: -0.132 [-14.845, 16.597], loss: 0.345947, mean_absolute_error: 0.539885, mean_q: -9.511971\n",
      " 13500/100000: episode: 27, duration: 8.369s, episode steps: 500, steps per second: 60, episode reward: -558.040, mean reward: -1.116 [-1.610, -0.210], mean action: 0.571 [-0.706, 1.397], mean observation: -0.340 [-14.279, 15.358], loss: 0.356740, mean_absolute_error: 0.549091, mean_q: -9.756248\n",
      " 14000/100000: episode: 28, duration: 8.223s, episode steps: 500, steps per second: 61, episode reward: -900.774, mean reward: -1.802 [-2.278, -0.483], mean action: 0.491 [-0.276, 1.549], mean observation: -0.191 [-1000.000, 1000.000], loss: 0.324771, mean_absolute_error: 0.521695, mean_q: -10.299067\n",
      " 14500/100000: episode: 29, duration: 7.012s, episode steps: 500, steps per second: 71, episode reward: -287.725, mean reward: -0.575 [-1.011, -0.389], mean action: 0.487 [-0.392, 1.605], mean observation: 0.073 [-830.770, 1000.000], loss: 0.329933, mean_absolute_error: 0.520383, mean_q: -10.526990\n",
      " 15000/100000: episode: 30, duration: 8.714s, episode steps: 500, steps per second: 57, episode reward: -422.923, mean reward: -0.846 [-1.068, -0.204], mean action: 0.297 [-0.795, 1.407], mean observation: -0.189 [-20.105, 14.718], loss: 0.337747, mean_absolute_error: 0.530172, mean_q: -10.732119\n",
      " 15500/100000: episode: 31, duration: 14.808s, episode steps: 500, steps per second: 34, episode reward: -1153.799, mean reward: -2.308 [-3.470, -0.398], mean action: 0.474 [-0.423, 1.469], mean observation: 0.166 [-1000.000, 1000.000], loss: 0.337325, mean_absolute_error: 0.531186, mean_q: -11.092279\n",
      " 16000/100000: episode: 32, duration: 9.437s, episode steps: 500, steps per second: 53, episode reward: -922.159, mean reward: -1.844 [-2.619, -0.310], mean action: 0.677 [-0.132, 1.464], mean observation: -0.293 [-1000.000, 1000.000], loss: 0.300345, mean_absolute_error: 0.496440, mean_q: -11.718192\n",
      " 16500/100000: episode: 33, duration: 24.332s, episode steps: 500, steps per second: 21, episode reward: -1045.152, mean reward: -2.090 [-3.626, -0.179], mean action: 0.331 [-1.274, 1.660], mean observation: -0.308 [-1000.000, 1000.000], loss: 0.317216, mean_absolute_error: 0.517216, mean_q: -12.465662\n",
      " 17000/100000: episode: 34, duration: 9.594s, episode steps: 500, steps per second: 52, episode reward: -266.340, mean reward: -0.533 [-0.916, -0.335], mean action: 0.350 [-0.445, 1.340], mean observation: -0.189 [-17.714, 9.177], loss: 0.289571, mean_absolute_error: 0.484858, mean_q: -12.732596\n",
      " 17500/100000: episode: 35, duration: 9.791s, episode steps: 500, steps per second: 51, episode reward: -1492.965, mean reward: -2.986 [-4.007, -0.037], mean action: 0.800 [-0.160, 1.562], mean observation: -0.113 [-12.760, 1000.000], loss: 0.313085, mean_absolute_error: 0.510255, mean_q: -13.153271\n",
      " 18000/100000: episode: 36, duration: 15.093s, episode steps: 500, steps per second: 33, episode reward: -1238.850, mean reward: -2.478 [-3.881, -0.322], mean action: 0.551 [-0.593, 1.662], mean observation: -0.253 [-13.895, 258.276], loss: 0.306163, mean_absolute_error: 0.510246, mean_q: -14.024467\n",
      " 18500/100000: episode: 37, duration: 9.709s, episode steps: 500, steps per second: 52, episode reward: -190.781, mean reward: -0.382 [-1.822, -0.147], mean action: 0.327 [-0.916, 1.667], mean observation: -0.018 [-12.498, 10.315], loss: 0.332844, mean_absolute_error: 0.538554, mean_q: -14.441385\n",
      " 19000/100000: episode: 38, duration: 9.578s, episode steps: 500, steps per second: 52, episode reward: -203.405, mean reward: -0.407 [-0.881, -0.088], mean action: 0.504 [-0.243, 1.212], mean observation: -0.201 [-12.862, 15.452], loss: 0.358196, mean_absolute_error: 0.566980, mean_q: -14.655029\n",
      " 19500/100000: episode: 39, duration: 8.112s, episode steps: 500, steps per second: 62, episode reward: -293.897, mean reward: -0.588 [-2.210, -0.293], mean action: 0.681 [-0.110, 1.546], mean observation: -0.188 [-1000.000, 617.458], loss: 0.358156, mean_absolute_error: 0.561132, mean_q: -14.526270\n",
      " 20000/100000: episode: 40, duration: 8.744s, episode steps: 500, steps per second: 57, episode reward: -284.098, mean reward: -0.568 [-1.581, -0.140], mean action: 0.442 [-0.576, 1.321], mean observation: -0.163 [-1000.000, 681.390], loss: 0.371693, mean_absolute_error: 0.575193, mean_q: -14.910313\n",
      " 20500/100000: episode: 41, duration: 8.213s, episode steps: 500, steps per second: 61, episode reward: -231.412, mean reward: -0.463 [-1.754, -0.047], mean action: 0.667 [-0.714, 1.669], mean observation: -0.142 [-1000.000, 1000.000], loss: 0.316045, mean_absolute_error: 0.515044, mean_q: -14.938172\n",
      " 21000/100000: episode: 42, duration: 10.159s, episode steps: 500, steps per second: 49, episode reward: -509.334, mean reward: -1.019 [-2.415, -0.046], mean action: 0.617 [-0.466, 1.627], mean observation: -0.123 [-523.544, 1000.000], loss: 0.344189, mean_absolute_error: 0.543618, mean_q: -14.916813\n",
      " 21500/100000: episode: 43, duration: 8.125s, episode steps: 500, steps per second: 62, episode reward: -137.097, mean reward: -0.274 [-1.292, -0.035], mean action: 0.410 [-0.600, 1.240], mean observation: -0.178 [-1000.000, 860.279], loss: 0.360489, mean_absolute_error: 0.561999, mean_q: -15.229898\n",
      " 22000/100000: episode: 44, duration: 11.905s, episode steps: 500, steps per second: 42, episode reward: -503.453, mean reward: -1.007 [-2.572, -0.221], mean action: 0.415 [-0.455, 1.504], mean observation: -0.653 [-1000.000, 1000.000], loss: 0.355812, mean_absolute_error: 0.561641, mean_q: -15.417193\n",
      " 22500/100000: episode: 45, duration: 13.728s, episode steps: 500, steps per second: 36, episode reward: -219.497, mean reward: -0.439 [-1.794, -0.027], mean action: 0.226 [-0.710, 1.248], mean observation: -0.629 [-1000.000, 1000.000], loss: 0.425132, mean_absolute_error: 0.629427, mean_q: -15.694017\n",
      " 23000/100000: episode: 46, duration: 12.996s, episode steps: 500, steps per second: 38, episode reward: -270.711, mean reward: -0.541 [-1.466, -0.153], mean action: 0.463 [-0.662, 1.631], mean observation: 0.744 [-1000.000, 1000.000], loss: 0.451543, mean_absolute_error: 0.664460, mean_q: -15.402476\n",
      " 23500/100000: episode: 47, duration: 15.697s, episode steps: 500, steps per second: 32, episode reward: -1244.013, mean reward: -2.488 [-3.877, -0.061], mean action: 0.473 [-0.628, 1.557], mean observation: -0.614 [-1000.000, 1000.000], loss: 0.582228, mean_absolute_error: 0.801019, mean_q: -16.172205\n",
      " 24000/100000: episode: 48, duration: 12.251s, episode steps: 500, steps per second: 41, episode reward: -360.434, mean reward: -0.721 [-1.324, -0.199], mean action: 0.484 [-0.486, 1.688], mean observation: 1.495 [-1000.000, 1000.000], loss: 0.744976, mean_absolute_error: 0.969317, mean_q: -16.461615\n",
      " 24500/100000: episode: 49, duration: 12.825s, episode steps: 500, steps per second: 39, episode reward: -339.330, mean reward: -0.679 [-1.506, -0.210], mean action: 0.263 [-0.571, 1.413], mean observation: 0.517 [-1000.000, 1000.000], loss: 0.764279, mean_absolute_error: 1.003370, mean_q: -16.675013\n",
      " 25000/100000: episode: 50, duration: 10.550s, episode steps: 500, steps per second: 47, episode reward: -181.517, mean reward: -0.363 [-0.908, -0.116], mean action: 0.529 [-0.312, 1.696], mean observation: -0.101 [-363.974, 1000.000], loss: 0.730848, mean_absolute_error: 0.961925, mean_q: -16.879274\n",
      " 25500/100000: episode: 51, duration: 9.241s, episode steps: 500, steps per second: 54, episode reward: -247.132, mean reward: -0.494 [-2.212, -0.059], mean action: 0.523 [-0.503, 1.665], mean observation: -0.104 [-1000.000, 1000.000], loss: 0.698164, mean_absolute_error: 0.935305, mean_q: -16.745293\n",
      " 26000/100000: episode: 52, duration: 9.227s, episode steps: 500, steps per second: 54, episode reward: -378.203, mean reward: -0.756 [-2.194, -0.074], mean action: 0.532 [-0.330, 1.816], mean observation: -0.330 [-1000.000, 1000.000], loss: 0.687082, mean_absolute_error: 0.932487, mean_q: -16.833956\n",
      " 26500/100000: episode: 53, duration: 9.189s, episode steps: 500, steps per second: 54, episode reward: -203.808, mean reward: -0.408 [-2.237, -0.023], mean action: 0.452 [-0.454, 1.524], mean observation: 0.134 [-795.500, 1000.000], loss: 0.707372, mean_absolute_error: 0.952546, mean_q: -17.152258\n",
      " 27000/100000: episode: 54, duration: 7.840s, episode steps: 500, steps per second: 64, episode reward: -102.616, mean reward: -0.205 [-1.485, -0.030], mean action: 0.497 [-0.337, 1.091], mean observation: -0.181 [-769.178, 1000.000], loss: 0.688831, mean_absolute_error: 0.931768, mean_q: -17.128208\n",
      " 27500/100000: episode: 55, duration: 10.928s, episode steps: 500, steps per second: 46, episode reward: -144.453, mean reward: -0.289 [-1.185, -0.111], mean action: 0.514 [-0.217, 1.427], mean observation: -0.135 [-43.060, 124.479], loss: 0.652285, mean_absolute_error: 0.886535, mean_q: -16.961163\n",
      " 28000/100000: episode: 56, duration: 11.306s, episode steps: 500, steps per second: 44, episode reward: -215.403, mean reward: -0.431 [-0.755, -0.137], mean action: 0.339 [-0.541, 1.302], mean observation: -0.260 [-918.775, 409.312], loss: 0.704746, mean_absolute_error: 0.944663, mean_q: -17.047407\n",
      " 28500/100000: episode: 57, duration: 8.754s, episode steps: 500, steps per second: 57, episode reward: -137.650, mean reward: -0.275 [-1.861, -0.066], mean action: 0.509 [-0.132, 1.265], mean observation: -0.092 [-1000.000, 1000.000], loss: 0.622546, mean_absolute_error: 0.863072, mean_q: -17.190054\n",
      " 29000/100000: episode: 58, duration: 11.040s, episode steps: 500, steps per second: 45, episode reward: -250.749, mean reward: -0.501 [-2.097, -0.165], mean action: 0.445 [-0.581, 1.474], mean observation: -0.029 [-396.911, 886.958], loss: 0.654402, mean_absolute_error: 0.891353, mean_q: -16.930172\n",
      " 29500/100000: episode: 59, duration: 8.731s, episode steps: 500, steps per second: 57, episode reward: -426.881, mean reward: -0.854 [-1.253, -0.100], mean action: 0.417 [-0.432, 1.254], mean observation: -0.246 [-1000.000, 870.427], loss: 0.641587, mean_absolute_error: 0.879016, mean_q: -17.242878\n",
      " 30000/100000: episode: 60, duration: 14.487s, episode steps: 500, steps per second: 35, episode reward: -101.376, mean reward: -0.203 [-2.143, -0.009], mean action: 0.683 [-0.285, 1.360], mean observation: -0.226 [-928.327, 494.394], loss: 0.643234, mean_absolute_error: 0.880116, mean_q: -17.342514\n",
      " 30500/100000: episode: 61, duration: 9.053s, episode steps: 500, steps per second: 55, episode reward: -356.422, mean reward: -0.713 [-1.433, -0.150], mean action: 0.348 [-0.444, 1.156], mean observation: -0.330 [-1000.000, 1000.000], loss: 0.587589, mean_absolute_error: 0.823260, mean_q: -17.620365\n",
      " 31000/100000: episode: 62, duration: 9.247s, episode steps: 500, steps per second: 54, episode reward: -198.408, mean reward: -0.397 [-0.826, -0.221], mean action: 0.516 [-0.126, 1.432], mean observation: -0.193 [-13.245, 6.560], loss: 0.589558, mean_absolute_error: 0.824765, mean_q: -17.147818\n",
      " 31500/100000: episode: 63, duration: 9.327s, episode steps: 500, steps per second: 54, episode reward: -133.142, mean reward: -0.266 [-2.503, -0.076], mean action: 0.353 [-0.624, 1.266], mean observation: -0.082 [-19.665, 9.642], loss: 0.629569, mean_absolute_error: 0.874738, mean_q: -17.518864\n",
      " 32000/100000: episode: 64, duration: 8.651s, episode steps: 500, steps per second: 58, episode reward: -108.925, mean reward: -0.218 [-1.796, -0.047], mean action: 0.693 [-0.298, 1.775], mean observation: -0.152 [-1000.000, 1000.000], loss: 0.598597, mean_absolute_error: 0.832717, mean_q: -17.336296\n",
      " 32500/100000: episode: 65, duration: 8.596s, episode steps: 500, steps per second: 58, episode reward: -86.417, mean reward: -0.173 [-2.185, -0.003], mean action: 0.385 [-0.538, 1.488], mean observation: -0.015 [-1000.000, 1000.000], loss: 0.591301, mean_absolute_error: 0.824908, mean_q: -17.340170\n",
      " 33000/100000: episode: 66, duration: 8.749s, episode steps: 500, steps per second: 57, episode reward: -189.507, mean reward: -0.379 [-1.539, -0.157], mean action: 0.603 [-0.363, 1.487], mean observation: -0.230 [-1000.000, 697.173], loss: 0.631475, mean_absolute_error: 0.873407, mean_q: -17.163733\n",
      " 33500/100000: episode: 67, duration: 7.584s, episode steps: 500, steps per second: 66, episode reward: -94.480, mean reward: -0.189 [-2.088, -0.012], mean action: 0.391 [-0.451, 1.226], mean observation: -0.025 [-629.228, 1000.000], loss: 0.590684, mean_absolute_error: 0.818977, mean_q: -17.105150\n",
      " 34000/100000: episode: 68, duration: 7.347s, episode steps: 500, steps per second: 68, episode reward: -238.770, mean reward: -0.478 [-1.082, -0.320], mean action: 0.471 [-0.497, 1.262], mean observation: -0.123 [-14.382, 8.466], loss: 0.616776, mean_absolute_error: 0.840986, mean_q: -17.101198\n",
      " 34500/100000: episode: 69, duration: 7.097s, episode steps: 500, steps per second: 70, episode reward: -268.685, mean reward: -0.537 [-1.826, -0.306], mean action: 0.525 [-0.405, 1.783], mean observation: -0.025 [-345.707, 298.154], loss: 0.560803, mean_absolute_error: 0.789043, mean_q: -17.161665\n",
      " 35000/100000: episode: 70, duration: 6.977s, episode steps: 500, steps per second: 72, episode reward: -204.379, mean reward: -0.409 [-0.859, -0.066], mean action: 0.435 [-0.272, 1.288], mean observation: -0.287 [-1000.000, 712.659], loss: 0.583514, mean_absolute_error: 0.810699, mean_q: -17.331953\n",
      " 35500/100000: episode: 71, duration: 8.660s, episode steps: 500, steps per second: 58, episode reward: -292.554, mean reward: -0.585 [-1.220, -0.043], mean action: 0.266 [-0.995, 1.202], mean observation: -0.227 [-14.855, 7.446], loss: 0.590410, mean_absolute_error: 0.807851, mean_q: -17.266520\n",
      " 36000/100000: episode: 72, duration: 8.359s, episode steps: 500, steps per second: 60, episode reward: -157.586, mean reward: -0.315 [-1.669, -0.141], mean action: 0.569 [-0.699, 1.304], mean observation: 0.143 [-19.221, 591.225], loss: 0.576350, mean_absolute_error: 0.800848, mean_q: -16.851246\n",
      " 36500/100000: episode: 73, duration: 7.268s, episode steps: 500, steps per second: 69, episode reward: -157.596, mean reward: -0.315 [-1.512, -0.058], mean action: 0.444 [-0.334, 1.134], mean observation: -0.075 [-35.613, 513.828], loss: 0.584058, mean_absolute_error: 0.802527, mean_q: -16.942244\n",
      " 37000/100000: episode: 74, duration: 8.705s, episode steps: 500, steps per second: 57, episode reward: -320.297, mean reward: -0.641 [-1.574, -0.320], mean action: 0.373 [-0.500, 1.129], mean observation: -0.112 [-18.229, 1000.000], loss: 0.588815, mean_absolute_error: 0.811891, mean_q: -17.061453\n",
      " 37500/100000: episode: 75, duration: 7.366s, episode steps: 500, steps per second: 68, episode reward: -125.748, mean reward: -0.251 [-1.961, -0.008], mean action: 0.735 [-0.202, 1.401], mean observation: -0.226 [-15.086, 8.860], loss: 0.523713, mean_absolute_error: 0.739173, mean_q: -17.064236\n",
      " 38000/100000: episode: 76, duration: 7.472s, episode steps: 500, steps per second: 67, episode reward: -185.018, mean reward: -0.370 [-2.207, -0.018], mean action: 0.775 [-0.135, 1.653], mean observation: -0.161 [-17.966, 10.656], loss: 0.579917, mean_absolute_error: 0.803816, mean_q: -16.740980\n",
      " 38500/100000: episode: 77, duration: 7.026s, episode steps: 500, steps per second: 71, episode reward: -104.436, mean reward: -0.209 [-1.788, -0.034], mean action: 0.515 [-0.296, 1.228], mean observation: -0.131 [-16.123, 10.765], loss: 0.584155, mean_absolute_error: 0.809176, mean_q: -17.013683\n",
      " 39000/100000: episode: 78, duration: 8.620s, episode steps: 500, steps per second: 58, episode reward: -161.396, mean reward: -0.323 [-2.010, -0.015], mean action: 0.692 [-0.482, 1.635], mean observation: -0.213 [-16.929, 9.130], loss: 0.614627, mean_absolute_error: 0.836352, mean_q: -16.941452\n",
      " 39500/100000: episode: 79, duration: 7.334s, episode steps: 500, steps per second: 68, episode reward: -133.844, mean reward: -0.268 [-1.251, -0.154], mean action: 0.725 [-0.227, 1.744], mean observation: -0.095 [-14.398, 125.854], loss: 0.527021, mean_absolute_error: 0.740003, mean_q: -16.758406\n",
      " 40000/100000: episode: 80, duration: 7.306s, episode steps: 500, steps per second: 68, episode reward: -185.968, mean reward: -0.372 [-2.069, -0.068], mean action: 0.629 [-0.248, 1.320], mean observation: -0.156 [-15.859, 9.678], loss: 0.637818, mean_absolute_error: 0.864305, mean_q: -17.040136\n",
      " 40500/100000: episode: 81, duration: 10.909s, episode steps: 500, steps per second: 46, episode reward: -134.501, mean reward: -0.269 [-2.537, -0.055], mean action: 0.599 [-0.624, 1.307], mean observation: -0.110 [-1000.000, 723.476], loss: 0.587741, mean_absolute_error: 0.804539, mean_q: -17.005156\n",
      " 41000/100000: episode: 82, duration: 8.153s, episode steps: 500, steps per second: 61, episode reward: -219.938, mean reward: -0.440 [-0.818, -0.067], mean action: 0.390 [-0.638, 1.490], mean observation: -0.182 [-14.618, 7.721], loss: 0.529589, mean_absolute_error: 0.750379, mean_q: -16.782827\n",
      " 41500/100000: episode: 83, duration: 9.529s, episode steps: 500, steps per second: 52, episode reward: -275.672, mean reward: -0.551 [-1.398, -0.123], mean action: 0.527 [-0.316, 1.402], mean observation: -0.288 [-16.013, 8.193], loss: 0.508127, mean_absolute_error: 0.727769, mean_q: -16.857317\n",
      " 42000/100000: episode: 84, duration: 6.922s, episode steps: 500, steps per second: 72, episode reward: -111.688, mean reward: -0.223 [-1.679, -0.064], mean action: 0.478 [-0.468, 1.475], mean observation: -0.095 [-1000.000, 1000.000], loss: 0.446231, mean_absolute_error: 0.657629, mean_q: -16.803122\n",
      " 42500/100000: episode: 85, duration: 10.630s, episode steps: 500, steps per second: 47, episode reward: -351.477, mean reward: -0.703 [-1.081, -0.371], mean action: 0.325 [-0.606, 1.107], mean observation: -0.173 [-14.207, 7.527], loss: 0.496133, mean_absolute_error: 0.716941, mean_q: -16.686699\n",
      " 43000/100000: episode: 86, duration: 7.236s, episode steps: 500, steps per second: 69, episode reward: -243.231, mean reward: -0.486 [-1.919, -0.294], mean action: 0.615 [-0.188, 1.804], mean observation: 0.044 [-16.403, 281.697], loss: 0.471674, mean_absolute_error: 0.685107, mean_q: -16.620541\n",
      " 43500/100000: episode: 87, duration: 6.230s, episode steps: 500, steps per second: 80, episode reward: -171.924, mean reward: -0.344 [-1.328, -0.214], mean action: 0.272 [-0.422, 1.200], mean observation: 0.097 [-105.996, 1000.000], loss: 0.526804, mean_absolute_error: 0.739311, mean_q: -16.477552\n",
      " 44000/100000: episode: 88, duration: 10.354s, episode steps: 500, steps per second: 48, episode reward: -339.934, mean reward: -0.680 [-1.044, -0.221], mean action: 0.313 [-0.572, 1.184], mean observation: -0.200 [-17.879, 10.158], loss: 0.526602, mean_absolute_error: 0.740263, mean_q: -16.690689\n",
      " 44500/100000: episode: 89, duration: 7.033s, episode steps: 500, steps per second: 71, episode reward: -166.111, mean reward: -0.332 [-1.937, -0.152], mean action: 0.656 [-0.281, 1.613], mean observation: -0.017 [-17.739, 8.135], loss: 0.541735, mean_absolute_error: 0.757527, mean_q: -16.531633\n",
      " 45000/100000: episode: 90, duration: 6.612s, episode steps: 500, steps per second: 76, episode reward: -98.135, mean reward: -0.196 [-1.695, -0.009], mean action: 0.567 [-0.726, 1.575], mean observation: 0.060 [-1000.000, 1000.000], loss: 0.494410, mean_absolute_error: 0.705362, mean_q: -16.653578\n",
      " 45500/100000: episode: 91, duration: 8.401s, episode steps: 500, steps per second: 60, episode reward: -104.601, mean reward: -0.209 [-1.242, -0.065], mean action: 0.473 [-0.686, 1.397], mean observation: -0.177 [-18.846, 8.408], loss: 0.506131, mean_absolute_error: 0.713742, mean_q: -16.355648\n",
      " 46000/100000: episode: 92, duration: 7.054s, episode steps: 500, steps per second: 71, episode reward: -68.693, mean reward: -0.137 [-1.836, -0.001], mean action: 0.758 [-0.135, 1.560], mean observation: -0.059 [-1000.000, 1000.000], loss: 0.487499, mean_absolute_error: 0.697404, mean_q: -16.093462\n",
      " 46500/100000: episode: 93, duration: 6.581s, episode steps: 500, steps per second: 76, episode reward: -112.212, mean reward: -0.224 [-1.748, -0.087], mean action: 0.634 [-0.212, 1.659], mean observation: -0.111 [-18.690, 8.588], loss: 0.523001, mean_absolute_error: 0.731771, mean_q: -16.022997\n",
      " 47000/100000: episode: 94, duration: 7.510s, episode steps: 500, steps per second: 67, episode reward: -180.304, mean reward: -0.361 [-2.094, -0.060], mean action: 0.368 [-0.313, 1.294], mean observation: 0.022 [-20.328, 9.327], loss: 0.502093, mean_absolute_error: 0.710923, mean_q: -16.110701\n",
      " 47500/100000: episode: 95, duration: 8.359s, episode steps: 500, steps per second: 60, episode reward: -173.901, mean reward: -0.348 [-1.485, -0.045], mean action: 0.723 [-0.360, 1.631], mean observation: -0.253 [-18.856, 8.429], loss: 0.514084, mean_absolute_error: 0.723966, mean_q: -16.085159\n",
      " 48000/100000: episode: 96, duration: 7.622s, episode steps: 500, steps per second: 66, episode reward: -100.074, mean reward: -0.200 [-2.192, -0.067], mean action: 0.615 [-0.212, 1.369], mean observation: -0.062 [-18.778, 8.654], loss: 0.506507, mean_absolute_error: 0.712928, mean_q: -16.180542\n",
      " 48500/100000: episode: 97, duration: 8.165s, episode steps: 500, steps per second: 61, episode reward: -194.694, mean reward: -0.389 [-1.883, -0.007], mean action: 0.670 [-0.618, 1.264], mean observation: -0.199 [-1000.000, 1000.000], loss: 0.491567, mean_absolute_error: 0.696112, mean_q: -15.920214\n",
      " 49000/100000: episode: 98, duration: 9.128s, episode steps: 500, steps per second: 55, episode reward: -139.336, mean reward: -0.279 [-1.226, -0.014], mean action: 0.552 [-0.659, 1.309], mean observation: -0.148 [-1000.000, 1000.000], loss: 0.481517, mean_absolute_error: 0.690938, mean_q: -15.770626\n",
      " 49500/100000: episode: 99, duration: 7.105s, episode steps: 500, steps per second: 70, episode reward: -109.594, mean reward: -0.219 [-1.299, -0.037], mean action: 0.588 [-0.185, 1.459], mean observation: -0.137 [-1000.000, 909.958], loss: 0.501277, mean_absolute_error: 0.709559, mean_q: -15.542713\n",
      " 50000/100000: episode: 100, duration: 12.296s, episode steps: 500, steps per second: 41, episode reward: -342.146, mean reward: -0.684 [-1.038, -0.417], mean action: 0.306 [-0.477, 1.564], mean observation: -0.179 [-16.588, 7.485], loss: 0.479333, mean_absolute_error: 0.681834, mean_q: -15.670707\n",
      " 50500/100000: episode: 101, duration: 7.688s, episode steps: 500, steps per second: 65, episode reward: -97.040, mean reward: -0.194 [-1.333, -0.014], mean action: 0.612 [-0.494, 1.624], mean observation: -0.193 [-18.274, 8.440], loss: 0.475261, mean_absolute_error: 0.684166, mean_q: -15.708424\n",
      " 51000/100000: episode: 102, duration: 6.363s, episode steps: 500, steps per second: 79, episode reward: -162.146, mean reward: -0.324 [-1.692, -0.213], mean action: 0.720 [-0.579, 1.775], mean observation: -0.086 [-1000.000, 846.484], loss: 0.439736, mean_absolute_error: 0.643793, mean_q: -15.861358\n",
      " 51500/100000: episode: 103, duration: 8.631s, episode steps: 500, steps per second: 58, episode reward: -63.465, mean reward: -0.127 [-1.896, -0.008], mean action: 0.714 [-0.382, 1.272], mean observation: 0.081 [-1000.000, 1000.000], loss: 0.471749, mean_absolute_error: 0.677437, mean_q: -15.777016\n",
      " 52000/100000: episode: 104, duration: 12.642s, episode steps: 500, steps per second: 40, episode reward: -721.810, mean reward: -1.444 [-2.688, -0.351], mean action: 0.128 [-0.996, 1.401], mean observation: -0.540 [-1000.000, 1000.000], loss: 0.438288, mean_absolute_error: 0.646265, mean_q: -15.501308\n",
      " 52500/100000: episode: 105, duration: 13.864s, episode steps: 500, steps per second: 36, episode reward: -552.032, mean reward: -1.104 [-2.035, -0.339], mean action: 0.336 [-0.710, 1.618], mean observation: -0.018 [-1000.000, 1000.000], loss: 0.450084, mean_absolute_error: 0.649839, mean_q: -15.820477\n",
      " 53000/100000: episode: 106, duration: 12.414s, episode steps: 500, steps per second: 40, episode reward: -267.183, mean reward: -0.534 [-1.877, -0.090], mean action: 0.275 [-0.543, 1.446], mean observation: 1.592 [-1000.000, 1000.000], loss: 0.504691, mean_absolute_error: 0.706128, mean_q: -15.660786\n",
      " 53500/100000: episode: 107, duration: 11.984s, episode steps: 500, steps per second: 42, episode reward: -318.451, mean reward: -0.637 [-1.680, -0.290], mean action: 0.422 [-0.386, 1.470], mean observation: 0.354 [-1000.000, 1000.000], loss: 0.521993, mean_absolute_error: 0.732273, mean_q: -15.868099\n",
      " 54000/100000: episode: 108, duration: 10.823s, episode steps: 500, steps per second: 46, episode reward: -133.196, mean reward: -0.266 [-1.932, -0.002], mean action: 0.298 [-0.575, 1.381], mean observation: 2.151 [-1000.000, 1000.000], loss: 0.591152, mean_absolute_error: 0.804445, mean_q: -15.860630\n",
      " 54500/100000: episode: 109, duration: 9.917s, episode steps: 500, steps per second: 50, episode reward: -125.360, mean reward: -0.251 [-1.893, -0.027], mean action: 0.285 [-0.377, 1.472], mean observation: 0.472 [-1000.000, 1000.000], loss: 0.630839, mean_absolute_error: 0.848006, mean_q: -15.617674\n",
      " 55000/100000: episode: 110, duration: 10.785s, episode steps: 500, steps per second: 46, episode reward: -130.902, mean reward: -0.262 [-1.911, -0.023], mean action: 0.165 [-0.945, 1.258], mean observation: -0.530 [-1000.000, 1000.000], loss: 0.711395, mean_absolute_error: 0.934371, mean_q: -15.789221\n",
      " 55500/100000: episode: 111, duration: 12.817s, episode steps: 500, steps per second: 39, episode reward: -333.655, mean reward: -0.667 [-1.420, -0.108], mean action: 0.142 [-0.763, 1.477], mean observation: 0.197 [-1000.000, 1000.000], loss: 0.772361, mean_absolute_error: 0.997378, mean_q: -15.759498\n",
      " 56000/100000: episode: 112, duration: 12.707s, episode steps: 500, steps per second: 39, episode reward: -355.581, mean reward: -0.711 [-1.743, -0.169], mean action: 0.229 [-0.592, 1.646], mean observation: 0.686 [-1000.000, 1000.000], loss: 0.703335, mean_absolute_error: 0.931204, mean_q: -15.680416\n",
      " 56500/100000: episode: 113, duration: 13.255s, episode steps: 500, steps per second: 38, episode reward: -406.122, mean reward: -0.812 [-1.565, -0.306], mean action: 0.273 [-0.520, 1.655], mean observation: 0.083 [-1000.000, 1000.000], loss: 0.776844, mean_absolute_error: 1.004843, mean_q: -15.934046\n",
      " 57000/100000: episode: 114, duration: 11.593s, episode steps: 500, steps per second: 43, episode reward: -219.940, mean reward: -0.440 [-2.709, -0.027], mean action: 0.063 [-1.099, 1.289], mean observation: 0.080 [-1000.000, 1000.000], loss: 0.744753, mean_absolute_error: 0.974956, mean_q: -15.805573\n",
      " 57500/100000: episode: 115, duration: 10.257s, episode steps: 500, steps per second: 49, episode reward: -117.497, mean reward: -0.235 [-1.991, -0.008], mean action: 0.340 [-0.416, 1.542], mean observation: -1.026 [-1000.000, 1000.000], loss: 0.767743, mean_absolute_error: 0.999136, mean_q: -15.986964\n",
      " 58000/100000: episode: 116, duration: 11.880s, episode steps: 500, steps per second: 42, episode reward: -317.969, mean reward: -0.636 [-1.085, -0.244], mean action: 0.081 [-0.574, 1.317], mean observation: 0.931 [-1000.000, 1000.000], loss: 0.783096, mean_absolute_error: 1.018209, mean_q: -16.088657\n",
      " 58500/100000: episode: 117, duration: 16.309s, episode steps: 500, steps per second: 31, episode reward: -1192.864, mean reward: -2.386 [-4.179, -0.363], mean action: 0.525 [-0.588, 1.930], mean observation: -1.194 [-1000.000, 1000.000], loss: 0.768164, mean_absolute_error: 1.006222, mean_q: -16.026226\n",
      " 59000/100000: episode: 118, duration: 7.856s, episode steps: 500, steps per second: 64, episode reward: -91.345, mean reward: -0.183 [-1.767, -0.064], mean action: 0.819 [-0.134, 1.369], mean observation: -0.150 [-1000.000, 1000.000], loss: 0.808677, mean_absolute_error: 1.043278, mean_q: -16.180784\n",
      " 59500/100000: episode: 119, duration: 8.412s, episode steps: 500, steps per second: 59, episode reward: -255.676, mean reward: -0.511 [-0.866, -0.323], mean action: 0.633 [-0.382, 1.437], mean observation: -0.168 [-16.322, 7.459], loss: 0.824097, mean_absolute_error: 1.059594, mean_q: -16.061975\n",
      " 60000/100000: episode: 120, duration: 7.579s, episode steps: 500, steps per second: 66, episode reward: -185.085, mean reward: -0.370 [-2.422, -0.054], mean action: 0.455 [-0.221, 1.266], mean observation: -0.105 [-714.772, 433.806], loss: 0.828039, mean_absolute_error: 1.068258, mean_q: -16.149523\n",
      " 60500/100000: episode: 121, duration: 7.290s, episode steps: 500, steps per second: 69, episode reward: -114.144, mean reward: -0.228 [-1.795, -0.097], mean action: 0.734 [-0.153, 1.595], mean observation: -0.114 [-1000.000, 1000.000], loss: 0.794934, mean_absolute_error: 1.032526, mean_q: -16.431347\n",
      " 61000/100000: episode: 122, duration: 7.505s, episode steps: 500, steps per second: 67, episode reward: -142.993, mean reward: -0.286 [-1.874, -0.131], mean action: 0.513 [-0.427, 1.499], mean observation: 0.126 [-42.779, 278.187], loss: 0.787546, mean_absolute_error: 1.021378, mean_q: -16.095627\n",
      " 61500/100000: episode: 123, duration: 11.373s, episode steps: 500, steps per second: 44, episode reward: -254.069, mean reward: -0.508 [-0.899, -0.265], mean action: 0.434 [-0.410, 2.102], mean observation: -0.196 [-114.664, 27.848], loss: 0.741957, mean_absolute_error: 0.977277, mean_q: -16.052801\n",
      " 62000/100000: episode: 124, duration: 8.500s, episode steps: 500, steps per second: 59, episode reward: -131.172, mean reward: -0.262 [-1.789, -0.043], mean action: 0.522 [-0.351, 1.221], mean observation: -0.071 [-1000.000, 1000.000], loss: 0.791887, mean_absolute_error: 1.027671, mean_q: -15.982070\n",
      " 62500/100000: episode: 125, duration: 8.389s, episode steps: 500, steps per second: 60, episode reward: -193.162, mean reward: -0.386 [-2.359, -0.101], mean action: 0.350 [-0.568, 1.520], mean observation: -0.023 [-86.623, 58.189], loss: 0.759917, mean_absolute_error: 0.994795, mean_q: -15.788901\n",
      " 63000/100000: episode: 126, duration: 8.639s, episode steps: 500, steps per second: 58, episode reward: -342.444, mean reward: -0.685 [-1.299, -0.189], mean action: 0.695 [-0.157, 1.326], mean observation: -0.285 [-19.569, 8.878], loss: 0.785551, mean_absolute_error: 1.022663, mean_q: -15.880075\n",
      " 63500/100000: episode: 127, duration: 8.663s, episode steps: 500, steps per second: 58, episode reward: -151.867, mean reward: -0.304 [-2.118, -0.070], mean action: 0.878 [-0.329, 1.891], mean observation: -0.110 [-1000.000, 1000.000], loss: 0.800557, mean_absolute_error: 1.034657, mean_q: -16.056530\n",
      " 64000/100000: episode: 128, duration: 9.904s, episode steps: 500, steps per second: 50, episode reward: -259.009, mean reward: -0.518 [-0.739, -0.339], mean action: 0.726 [-0.198, 1.722], mean observation: -0.136 [-1000.000, 1000.000], loss: 0.757943, mean_absolute_error: 0.992272, mean_q: -15.938210\n",
      " 64500/100000: episode: 129, duration: 7.053s, episode steps: 500, steps per second: 71, episode reward: -72.161, mean reward: -0.144 [-1.778, -0.017], mean action: 0.507 [-0.383, 1.338], mean observation: 0.028 [-468.819, 1000.000], loss: 0.755732, mean_absolute_error: 0.990359, mean_q: -15.983244\n",
      " 65000/100000: episode: 130, duration: 10.088s, episode steps: 500, steps per second: 50, episode reward: -185.450, mean reward: -0.371 [-1.195, -0.266], mean action: 0.447 [-0.732, 1.875], mean observation: -0.143 [-1000.000, 1000.000], loss: 0.773066, mean_absolute_error: 1.009448, mean_q: -15.967695\n",
      " 65500/100000: episode: 131, duration: 8.461s, episode steps: 500, steps per second: 59, episode reward: -269.157, mean reward: -0.538 [-1.648, -0.116], mean action: 0.764 [-0.289, 1.483], mean observation: -0.287 [-17.678, 8.198], loss: 0.714361, mean_absolute_error: 0.941540, mean_q: -15.804931\n",
      " 66000/100000: episode: 132, duration: 9.525s, episode steps: 500, steps per second: 52, episode reward: -236.157, mean reward: -0.472 [-1.260, -0.134], mean action: 0.714 [-0.492, 1.477], mean observation: -0.222 [-987.957, 1000.000], loss: 0.784997, mean_absolute_error: 1.020973, mean_q: -16.222574\n",
      " 66500/100000: episode: 133, duration: 6.780s, episode steps: 500, steps per second: 74, episode reward: -145.101, mean reward: -0.290 [-1.452, -0.053], mean action: 0.534 [-0.275, 1.242], mean observation: -0.096 [-18.772, 8.678], loss: 0.817859, mean_absolute_error: 1.048654, mean_q: -16.152384\n",
      " 67000/100000: episode: 134, duration: 6.600s, episode steps: 500, steps per second: 76, episode reward: -61.808, mean reward: -0.124 [-1.747, -0.007], mean action: 0.599 [-0.307, 1.346], mean observation: -0.196 [-1000.000, 643.250], loss: 0.727403, mean_absolute_error: 0.954143, mean_q: -15.989850\n",
      " 67500/100000: episode: 135, duration: 9.117s, episode steps: 500, steps per second: 55, episode reward: -99.660, mean reward: -0.199 [-1.955, -0.054], mean action: 0.790 [-0.460, 1.565], mean observation: -0.242 [-1000.000, 622.874], loss: 0.711514, mean_absolute_error: 0.945271, mean_q: -15.741943\n",
      " 68000/100000: episode: 136, duration: 7.382s, episode steps: 500, steps per second: 68, episode reward: -134.465, mean reward: -0.269 [-2.175, -0.080], mean action: 0.470 [-0.372, 1.698], mean observation: -0.010 [-1000.000, 1000.000], loss: 0.747895, mean_absolute_error: 0.980218, mean_q: -15.792717\n",
      " 68500/100000: episode: 137, duration: 9.012s, episode steps: 500, steps per second: 55, episode reward: -193.227, mean reward: -0.386 [-1.480, -0.212], mean action: 0.231 [-0.428, 1.371], mean observation: -0.143 [-1000.000, 743.546], loss: 0.700632, mean_absolute_error: 0.930004, mean_q: -15.800376\n",
      " 69000/100000: episode: 138, duration: 8.490s, episode steps: 500, steps per second: 59, episode reward: -219.601, mean reward: -0.439 [-1.016, -0.230], mean action: 0.412 [-0.281, 1.354], mean observation: -0.067 [-1000.000, 1000.000], loss: 0.673371, mean_absolute_error: 0.905191, mean_q: -15.870071\n",
      " 69500/100000: episode: 139, duration: 9.393s, episode steps: 500, steps per second: 53, episode reward: -229.265, mean reward: -0.459 [-0.902, -0.195], mean action: 0.431 [-0.287, 1.366], mean observation: -0.210 [-1000.000, 1000.000], loss: 0.696570, mean_absolute_error: 0.922238, mean_q: -15.883983\n",
      " 70000/100000: episode: 140, duration: 7.865s, episode steps: 500, steps per second: 64, episode reward: -185.962, mean reward: -0.372 [-2.339, -0.016], mean action: 0.495 [-0.567, 1.326], mean observation: -0.130 [-1000.000, 849.377], loss: 0.707460, mean_absolute_error: 0.933220, mean_q: -16.156105\n",
      " 70500/100000: episode: 141, duration: 8.966s, episode steps: 500, steps per second: 56, episode reward: -185.570, mean reward: -0.371 [-1.392, -0.224], mean action: 0.283 [-0.587, 1.664], mean observation: -0.106 [-273.392, 528.139], loss: 0.754146, mean_absolute_error: 0.979455, mean_q: -15.729502\n",
      " 71000/100000: episode: 142, duration: 8.057s, episode steps: 500, steps per second: 62, episode reward: -85.957, mean reward: -0.172 [-1.444, -0.065], mean action: 0.409 [-0.355, 1.201], mean observation: -0.244 [-1000.000, 662.705], loss: 0.672261, mean_absolute_error: 0.894880, mean_q: -15.708715\n",
      " 71500/100000: episode: 143, duration: 8.201s, episode steps: 500, steps per second: 61, episode reward: -227.226, mean reward: -0.454 [-0.724, -0.228], mean action: 0.427 [-0.622, 1.235], mean observation: -0.197 [-19.293, 8.826], loss: 0.681253, mean_absolute_error: 0.907420, mean_q: -15.764779\n",
      " 72000/100000: episode: 144, duration: 6.595s, episode steps: 500, steps per second: 76, episode reward: -160.527, mean reward: -0.321 [-1.649, -0.124], mean action: 0.425 [-0.548, 1.365], mean observation: -0.075 [-17.187, 8.008], loss: 0.712747, mean_absolute_error: 0.940563, mean_q: -15.830024\n",
      " 72500/100000: episode: 145, duration: 7.914s, episode steps: 500, steps per second: 63, episode reward: -89.724, mean reward: -0.179 [-1.073, -0.024], mean action: 0.563 [-0.667, 1.526], mean observation: -0.092 [-423.728, 956.955], loss: 0.691902, mean_absolute_error: 0.915523, mean_q: -15.764934\n",
      " 73000/100000: episode: 146, duration: 8.218s, episode steps: 500, steps per second: 61, episode reward: -239.727, mean reward: -0.479 [-1.147, -0.291], mean action: 0.558 [-0.416, 1.506], mean observation: -0.185 [-1000.000, 1000.000], loss: 0.698573, mean_absolute_error: 0.926872, mean_q: -15.605968\n",
      " 73500/100000: episode: 147, duration: 7.307s, episode steps: 500, steps per second: 68, episode reward: -125.441, mean reward: -0.251 [-1.816, -0.100], mean action: 0.729 [-0.170, 1.652], mean observation: 0.029 [-1000.000, 1000.000], loss: 0.650344, mean_absolute_error: 0.876817, mean_q: -15.685463\n",
      " 74000/100000: episode: 148, duration: 8.437s, episode steps: 500, steps per second: 59, episode reward: -102.470, mean reward: -0.205 [-2.297, -0.053], mean action: 0.784 [-0.192, 1.591], mean observation: -0.037 [-1000.000, 1000.000], loss: 0.696736, mean_absolute_error: 0.930573, mean_q: -15.813736\n",
      " 74500/100000: episode: 149, duration: 8.609s, episode steps: 500, steps per second: 58, episode reward: -133.829, mean reward: -0.268 [-1.198, -0.119], mean action: 0.520 [-0.376, 1.591], mean observation: -0.061 [-17.969, 1000.000], loss: 0.703161, mean_absolute_error: 0.931825, mean_q: -15.716750\n",
      " 75000/100000: episode: 150, duration: 8.503s, episode steps: 500, steps per second: 59, episode reward: -91.783, mean reward: -0.184 [-1.282, -0.033], mean action: 0.776 [-0.157, 1.459], mean observation: -0.143 [-17.186, 8.098], loss: 0.668304, mean_absolute_error: 0.890691, mean_q: -15.837740\n",
      " 75500/100000: episode: 151, duration: 8.233s, episode steps: 500, steps per second: 61, episode reward: -87.387, mean reward: -0.175 [-1.623, -0.053], mean action: 0.633 [-0.276, 1.400], mean observation: -0.086 [-1000.000, 1000.000], loss: 0.674720, mean_absolute_error: 0.899871, mean_q: -15.653035\n",
      " 76000/100000: episode: 152, duration: 8.608s, episode steps: 500, steps per second: 58, episode reward: -143.215, mean reward: -0.286 [-1.414, -0.042], mean action: 0.574 [-0.219, 1.486], mean observation: -0.273 [-1000.000, 673.993], loss: 0.665906, mean_absolute_error: 0.887893, mean_q: -15.599325\n",
      " 76500/100000: episode: 153, duration: 6.776s, episode steps: 500, steps per second: 74, episode reward: -119.879, mean reward: -0.240 [-1.202, -0.080], mean action: 0.458 [-0.514, 1.536], mean observation: -0.111 [-1000.000, 726.817], loss: 0.646335, mean_absolute_error: 0.865590, mean_q: -15.832309\n",
      " 77000/100000: episode: 154, duration: 6.763s, episode steps: 500, steps per second: 74, episode reward: -66.953, mean reward: -0.134 [-1.457, -0.004], mean action: 0.591 [-0.284, 1.406], mean observation: -0.120 [-19.285, 8.601], loss: 0.669032, mean_absolute_error: 0.889313, mean_q: -15.481560\n",
      " 77500/100000: episode: 155, duration: 6.631s, episode steps: 500, steps per second: 75, episode reward: -189.185, mean reward: -0.378 [-1.612, -0.152], mean action: 0.569 [-0.103, 1.605], mean observation: -0.076 [-16.678, 7.978], loss: 0.655377, mean_absolute_error: 0.876758, mean_q: -15.478993\n",
      " 78000/100000: episode: 156, duration: 7.366s, episode steps: 500, steps per second: 68, episode reward: -94.133, mean reward: -0.188 [-1.361, -0.082], mean action: 0.319 [-0.300, 1.166], mean observation: -0.105 [-1000.000, 1000.000], loss: 0.657230, mean_absolute_error: 0.875694, mean_q: -15.444716\n",
      " 78500/100000: episode: 157, duration: 7.967s, episode steps: 500, steps per second: 63, episode reward: -309.375, mean reward: -0.619 [-1.150, -0.060], mean action: 0.291 [-0.849, 1.287], mean observation: -0.284 [-1000.000, 1000.000], loss: 0.660550, mean_absolute_error: 0.877663, mean_q: -15.478068\n",
      " 79000/100000: episode: 158, duration: 6.739s, episode steps: 500, steps per second: 74, episode reward: -94.871, mean reward: -0.190 [-1.864, -0.086], mean action: 0.401 [-0.534, 1.388], mean observation: -0.084 [-1000.000, 566.412], loss: 0.653791, mean_absolute_error: 0.873518, mean_q: -15.264313\n",
      " 79500/100000: episode: 159, duration: 7.687s, episode steps: 500, steps per second: 65, episode reward: -141.153, mean reward: -0.282 [-2.610, -0.026], mean action: 0.599 [-0.501, 1.364], mean observation: -0.115 [-1000.000, 1000.000], loss: 0.695366, mean_absolute_error: 0.916272, mean_q: -15.212410\n",
      " 80000/100000: episode: 160, duration: 8.255s, episode steps: 500, steps per second: 61, episode reward: -76.948, mean reward: -0.154 [-2.099, -0.000], mean action: 0.404 [-0.533, 1.238], mean observation: -0.048 [-1000.000, 1000.000], loss: 0.650676, mean_absolute_error: 0.872616, mean_q: -15.338466\n",
      " 80500/100000: episode: 161, duration: 8.126s, episode steps: 500, steps per second: 62, episode reward: -79.877, mean reward: -0.160 [-1.939, -0.050], mean action: 0.985 [-0.097, 1.583], mean observation: -0.193 [-1000.000, 678.016], loss: 0.694824, mean_absolute_error: 0.913842, mean_q: -15.440030\n",
      " 81000/100000: episode: 162, duration: 8.896s, episode steps: 500, steps per second: 56, episode reward: -226.356, mean reward: -0.453 [-0.914, -0.251], mean action: 0.582 [-0.451, 1.781], mean observation: -0.213 [-1000.000, 1000.000], loss: 0.639305, mean_absolute_error: 0.854371, mean_q: -15.113761\n",
      " 81500/100000: episode: 163, duration: 8.945s, episode steps: 500, steps per second: 56, episode reward: -135.913, mean reward: -0.272 [-1.017, -0.110], mean action: 0.459 [-0.340, 1.247], mean observation: -0.190 [-21.193, 9.459], loss: 0.677443, mean_absolute_error: 0.897066, mean_q: -15.379471\n",
      " 82000/100000: episode: 164, duration: 7.282s, episode steps: 500, steps per second: 69, episode reward: -77.331, mean reward: -0.155 [-1.129, -0.001], mean action: 0.649 [-0.161, 1.367], mean observation: -0.189 [-1000.000, 839.669], loss: 0.677943, mean_absolute_error: 0.892546, mean_q: -15.340385\n",
      " 82500/100000: episode: 165, duration: 7.002s, episode steps: 500, steps per second: 71, episode reward: -91.085, mean reward: -0.182 [-1.944, -0.015], mean action: 0.617 [-0.439, 1.188], mean observation: -0.195 [-17.936, 10.287], loss: 0.637944, mean_absolute_error: 0.855288, mean_q: -15.187657\n",
      " 83000/100000: episode: 166, duration: 8.684s, episode steps: 500, steps per second: 58, episode reward: -98.497, mean reward: -0.197 [-2.151, -0.021], mean action: 0.509 [-0.541, 1.607], mean observation: -0.079 [-746.038, 556.470], loss: 0.657339, mean_absolute_error: 0.868484, mean_q: -15.003219\n",
      " 83500/100000: episode: 167, duration: 7.825s, episode steps: 500, steps per second: 64, episode reward: -148.196, mean reward: -0.296 [-2.163, -0.019], mean action: 0.528 [-0.265, 1.459], mean observation: -0.009 [-17.081, 7.981], loss: 0.645782, mean_absolute_error: 0.857087, mean_q: -14.988593\n",
      " 84000/100000: episode: 168, duration: 7.666s, episode steps: 500, steps per second: 65, episode reward: -57.902, mean reward: -0.116 [-2.279, -0.001], mean action: 0.565 [-0.328, 1.282], mean observation: -0.122 [-15.226, 10.829], loss: 0.681061, mean_absolute_error: 0.897672, mean_q: -15.256091\n",
      " 84500/100000: episode: 169, duration: 7.967s, episode steps: 500, steps per second: 63, episode reward: -326.866, mean reward: -0.654 [-0.850, -0.272], mean action: 0.508 [-0.321, 1.340], mean observation: -0.191 [-1000.000, 1000.000], loss: 0.622135, mean_absolute_error: 0.833520, mean_q: -14.757922\n",
      " 85000/100000: episode: 170, duration: 7.098s, episode steps: 500, steps per second: 70, episode reward: -65.098, mean reward: -0.130 [-2.022, -0.002], mean action: 0.679 [-0.243, 1.639], mean observation: -0.185 [-14.202, 6.974], loss: 0.659331, mean_absolute_error: 0.876261, mean_q: -14.960175\n",
      " 85500/100000: episode: 171, duration: 8.364s, episode steps: 500, steps per second: 60, episode reward: -125.238, mean reward: -0.250 [-2.112, -0.021], mean action: 0.546 [-0.484, 1.720], mean observation: -0.011 [-1000.000, 1000.000], loss: 0.617710, mean_absolute_error: 0.824432, mean_q: -14.993855\n",
      " 86000/100000: episode: 172, duration: 7.518s, episode steps: 500, steps per second: 67, episode reward: -93.367, mean reward: -0.187 [-1.894, -0.037], mean action: 0.764 [-0.528, 1.609], mean observation: -0.201 [-1000.000, 891.905], loss: 0.619872, mean_absolute_error: 0.828490, mean_q: -14.966081\n",
      " 86500/100000: episode: 173, duration: 7.257s, episode steps: 500, steps per second: 69, episode reward: -196.954, mean reward: -0.394 [-1.457, -0.186], mean action: 0.218 [-0.792, 1.279], mean observation: -0.118 [-824.177, 583.950], loss: 0.641497, mean_absolute_error: 0.847493, mean_q: -15.024053\n",
      " 87000/100000: episode: 174, duration: 9.030s, episode steps: 500, steps per second: 55, episode reward: -283.235, mean reward: -0.566 [-0.998, -0.328], mean action: 0.322 [-0.690, 1.400], mean observation: -0.200 [-18.004, 8.018], loss: 0.647950, mean_absolute_error: 0.854458, mean_q: -15.097539\n",
      " 87500/100000: episode: 175, duration: 8.448s, episode steps: 500, steps per second: 59, episode reward: -130.146, mean reward: -0.260 [-1.405, -0.077], mean action: 0.610 [-0.309, 1.509], mean observation: -0.217 [-1000.000, 1000.000], loss: 0.653006, mean_absolute_error: 0.853738, mean_q: -15.155234\n",
      " 88000/100000: episode: 176, duration: 8.175s, episode steps: 500, steps per second: 61, episode reward: -247.047, mean reward: -0.494 [-1.687, -0.216], mean action: 0.464 [-0.323, 1.413], mean observation: -0.065 [-1000.000, 887.712], loss: 0.645019, mean_absolute_error: 0.854214, mean_q: -15.109321\n",
      " 88500/100000: episode: 177, duration: 8.571s, episode steps: 500, steps per second: 58, episode reward: -219.043, mean reward: -0.438 [-0.779, -0.332], mean action: 0.444 [-0.588, 1.589], mean observation: -0.220 [-1000.000, 917.178], loss: 0.591115, mean_absolute_error: 0.794047, mean_q: -15.131111\n",
      " 89000/100000: episode: 178, duration: 7.134s, episode steps: 500, steps per second: 70, episode reward: -110.265, mean reward: -0.221 [-2.111, -0.062], mean action: 0.590 [-0.380, 1.463], mean observation: -0.046 [-1000.000, 1000.000], loss: 0.669398, mean_absolute_error: 0.877321, mean_q: -15.270670\n",
      " 89500/100000: episode: 179, duration: 8.081s, episode steps: 500, steps per second: 62, episode reward: -80.479, mean reward: -0.161 [-1.217, -0.010], mean action: 0.532 [-0.489, 1.477], mean observation: -0.157 [-1000.000, 908.910], loss: 0.647694, mean_absolute_error: 0.852239, mean_q: -14.957848\n",
      " 90000/100000: episode: 180, duration: 8.713s, episode steps: 500, steps per second: 57, episode reward: -135.479, mean reward: -0.271 [-1.637, -0.181], mean action: 0.447 [-0.265, 1.184], mean observation: -0.117 [-19.788, 8.645], loss: 0.616799, mean_absolute_error: 0.823790, mean_q: -14.978002\n",
      " 90500/100000: episode: 181, duration: 7.761s, episode steps: 500, steps per second: 64, episode reward: -53.690, mean reward: -0.107 [-2.077, -0.016], mean action: 0.762 [-0.192, 1.720], mean observation: -0.010 [-1000.000, 1000.000], loss: 0.660348, mean_absolute_error: 0.865638, mean_q: -14.979900\n",
      " 91000/100000: episode: 182, duration: 8.072s, episode steps: 500, steps per second: 62, episode reward: -70.001, mean reward: -0.140 [-1.877, -0.002], mean action: 0.680 [-0.210, 1.300], mean observation: -0.189 [-1000.000, 961.728], loss: 0.661631, mean_absolute_error: 0.863892, mean_q: -14.897996\n",
      " 91500/100000: episode: 183, duration: 7.620s, episode steps: 500, steps per second: 66, episode reward: -70.652, mean reward: -0.141 [-1.920, -0.037], mean action: 0.698 [-0.144, 1.600], mean observation: -0.109 [-1000.000, 1000.000], loss: 0.607892, mean_absolute_error: 0.814234, mean_q: -15.089598\n",
      " 92000/100000: episode: 184, duration: 7.846s, episode steps: 500, steps per second: 64, episode reward: -294.359, mean reward: -0.589 [-1.069, -0.079], mean action: 0.552 [-0.327, 1.426], mean observation: -0.237 [-19.594, 8.624], loss: 0.609166, mean_absolute_error: 0.812970, mean_q: -15.230998\n",
      " 92500/100000: episode: 185, duration: 7.195s, episode steps: 500, steps per second: 69, episode reward: -224.724, mean reward: -0.449 [-1.485, -0.165], mean action: 0.113 [-0.797, 1.094], mean observation: -0.094 [-1000.000, 897.329], loss: 0.610862, mean_absolute_error: 0.812715, mean_q: -15.053567\n",
      " 93000/100000: episode: 186, duration: 9.598s, episode steps: 500, steps per second: 52, episode reward: -123.764, mean reward: -0.248 [-2.500, -0.025], mean action: 0.661 [-0.468, 1.444], mean observation: 0.022 [-1000.000, 1000.000], loss: 0.620621, mean_absolute_error: 0.823338, mean_q: -15.054287\n",
      " 93500/100000: episode: 187, duration: 7.479s, episode steps: 500, steps per second: 67, episode reward: -119.534, mean reward: -0.239 [-1.479, -0.068], mean action: 0.785 [-0.210, 1.597], mean observation: -0.161 [-1000.000, 1000.000], loss: 0.641454, mean_absolute_error: 0.841945, mean_q: -15.089679\n",
      " 94000/100000: episode: 188, duration: 10.165s, episode steps: 500, steps per second: 49, episode reward: -255.855, mean reward: -0.512 [-0.937, -0.405], mean action: 0.194 [-0.482, 1.615], mean observation: -0.187 [-1000.000, 925.370], loss: 0.604283, mean_absolute_error: 0.801507, mean_q: -15.063927\n",
      " 94500/100000: episode: 189, duration: 7.678s, episode steps: 500, steps per second: 65, episode reward: -294.696, mean reward: -0.589 [-1.310, -0.079], mean action: 0.305 [-0.436, 1.279], mean observation: -0.288 [-1000.000, 877.283], loss: 0.622874, mean_absolute_error: 0.827855, mean_q: -15.184373\n",
      " 95000/100000: episode: 190, duration: 8.003s, episode steps: 500, steps per second: 62, episode reward: -69.438, mean reward: -0.139 [-1.443, -0.002], mean action: 0.819 [-0.371, 1.658], mean observation: -0.156 [-18.041, 8.163], loss: 0.598124, mean_absolute_error: 0.796548, mean_q: -15.128172\n",
      " 95500/100000: episode: 191, duration: 8.794s, episode steps: 500, steps per second: 57, episode reward: -243.382, mean reward: -0.487 [-1.844, -0.067], mean action: 0.644 [-0.359, 1.433], mean observation: -0.081 [-1000.000, 704.416], loss: 0.661292, mean_absolute_error: 0.863418, mean_q: -15.217440\n",
      " 96000/100000: episode: 192, duration: 7.450s, episode steps: 500, steps per second: 67, episode reward: -78.678, mean reward: -0.157 [-1.325, -0.020], mean action: 0.548 [-0.550, 1.293], mean observation: -0.130 [-18.059, 8.184], loss: 0.558044, mean_absolute_error: 0.756283, mean_q: -15.358330\n",
      " 96500/100000: episode: 193, duration: 7.772s, episode steps: 500, steps per second: 64, episode reward: -236.628, mean reward: -0.473 [-1.326, -0.058], mean action: 0.407 [-0.428, 1.261], mean observation: -0.251 [-19.958, 8.857], loss: 0.591410, mean_absolute_error: 0.789373, mean_q: -15.249493\n",
      " 97000/100000: episode: 194, duration: 7.177s, episode steps: 500, steps per second: 70, episode reward: -133.589, mean reward: -0.267 [-1.471, -0.038], mean action: 0.459 [-0.285, 1.275], mean observation: -0.082 [-17.714, 7.889], loss: 0.581518, mean_absolute_error: 0.776856, mean_q: -15.178279\n",
      " 97500/100000: episode: 195, duration: 7.038s, episode steps: 500, steps per second: 71, episode reward: -119.904, mean reward: -0.240 [-1.986, -0.018], mean action: 0.249 [-0.616, 1.293], mean observation: 0.018 [-17.743, 9.130], loss: 0.608915, mean_absolute_error: 0.803524, mean_q: -15.087090\n",
      " 98000/100000: episode: 196, duration: 7.559s, episode steps: 500, steps per second: 66, episode reward: -104.133, mean reward: -0.208 [-1.664, -0.001], mean action: 0.680 [-0.268, 1.606], mean observation: -0.257 [-1000.000, 871.931], loss: 0.599977, mean_absolute_error: 0.798636, mean_q: -15.116545\n",
      " 98500/100000: episode: 197, duration: 10.147s, episode steps: 500, steps per second: 49, episode reward: -139.753, mean reward: -0.280 [-1.527, -0.038], mean action: 0.587 [-0.368, 1.420], mean observation: -0.222 [-1000.000, 1000.000], loss: 0.590512, mean_absolute_error: 0.790026, mean_q: -15.192871\n",
      " 99000/100000: episode: 198, duration: 8.464s, episode steps: 500, steps per second: 59, episode reward: -116.510, mean reward: -0.233 [-1.607, -0.061], mean action: 0.665 [-0.497, 1.559], mean observation: -0.044 [-1000.000, 1000.000], loss: 0.637451, mean_absolute_error: 0.839525, mean_q: -15.231961\n",
      " 99500/100000: episode: 199, duration: 9.948s, episode steps: 500, steps per second: 50, episode reward: -145.354, mean reward: -0.291 [-0.957, -0.135], mean action: 0.471 [-0.472, 1.557], mean observation: -0.191 [-1000.000, 1000.000], loss: 0.587241, mean_absolute_error: 0.788772, mean_q: -15.158750\n",
      " 100000/100000: episode: 200, duration: 9.715s, episode steps: 500, steps per second: 51, episode reward: -141.576, mean reward: -0.283 [-1.045, -0.182], mean action: 0.541 [-0.576, 1.768], mean observation: -0.156 [-1000.000, 1000.000], loss: 0.615253, mean_absolute_error: 0.813783, mean_q: -15.329379\n",
      "done, took 1864.796 seconds\n"
     ]
    }
   ],
   "source": [
    "nallsteps=10000\n",
    "#Warning: verbose=1 freezes the notebook\n",
    "agent.fit(env, nb_steps=nallsteps, visualize=False, verbose=2, nb_max_episode_steps=env.timestep_limit, log_interval=10000)\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights(args.model, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -91.295, steps: 500\n",
      "Episode 2: reward: -38.727, steps: 500\n",
      "Episode 3: reward: -40.374, steps: 500\n",
      "Episode 4: reward: -198.706, steps: 500\n",
      "Episode 5: reward: -168.487, steps: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdce5d425d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load_weights(args.model)\n",
    "# Finally, evaluate our algorithm for 1 episode.\n",
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opensim-rl)",
   "language": "python",
   "name": "opensim-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
