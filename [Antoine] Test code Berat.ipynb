{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# %% Derived from keras-rl\n",
    "import opensim as osim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import keras \n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, merge\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "sys.path = [\"../\"]+sys.path\n",
    "from osim.env import *\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import matplotlib.pylab as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import opensim\n",
    "\n",
    "#print(opensim.__path__)\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from osim.env import OsimEnv\n",
    "\n",
    "#import .osim\n",
    "#print(OsimEnv.__path__)\n",
    "\n",
    "class ArmEnv(OsimEnv):\n",
    "    ninput = 14\n",
    "    #model_path = os.path.join(os.path.dirname(__file__), './models/arm2dof6musc.osim')\n",
    "    model_path = './models/arm2dof6musc.osim'\n",
    "    #print(dir(OsimEnv))\n",
    "\n",
    "    def __init__(self, visualize = False):\n",
    "        self.iepisode = 0\n",
    "        self.shoulder = 0.0\n",
    "        self.elbow = 0.0\n",
    "        super(ArmEnv, self).__init__(visualize = visualize)\n",
    "\n",
    "    def configure(self):\n",
    "        super(ArmEnv, self).configure()\n",
    "        self.osim_model.joints.append(opensim.CustomJoint.safeDownCast(self.osim_model.jointSet.get(0)))\n",
    "        self.osim_model.joints.append(opensim.CustomJoint.safeDownCast(self.osim_model.jointSet.get(1)))\n",
    "\n",
    "    def new_target(self):\n",
    "        #self.shoulder = -0.02275097#random.uniform(-1.2,0.3)\n",
    "        #self.elbow = 0.2195816#random.uniform(-1.0,0)\n",
    "        self.shoulder = 0.0#random.uniform(-1.2,0.3)\n",
    "        self.elbow = 0.0#random.uniform(-1.0,0)\n",
    "        #print(self.osim_model.joints[0].getCoordinate(0).getValue(self.osim_model.state))\n",
    "        #print(self.osim_model.joints[0].getCoordinate(0).getValue(self.osim_model.state))\n",
    "        #print(dir(self.osim_model.joints[0].set_coordinates))\n",
    "        #print(self.osim_model.joints[0].set_coordinates.__doc__)\n",
    "        #self.osim_model.joints[0].set_coordinates(0)\n",
    "        #print(dir(self.osim_model.joints[0]))\n",
    "        #self.osim_model.joints[1].set_coordinates(0,0.0)\n",
    "        #action = [0,0,0,0,0,0]\n",
    "        #muscle = self.osim_model.muscleSet.get(0)\n",
    "        #muscle.setActivation(self.osim_model.state, float(action[0]))\n",
    "        #muscleSet = self.osim_model.model.getMuscles()\n",
    "        #muscle = muscleSet.get(0)\n",
    "        #muscle.setActivation(self.osim_model.state, float(action[0]))\n",
    "    \n",
    "\n",
    "    def new_new(self):\n",
    "        print(self.osim_model.joints[0].getCoordinate(0).getValue(self.osim_model.state))\n",
    "        #self.osim_model.joints[0].getCoordinate(0).setValue(self.osim_model.state, -0.02275097)\n",
    "        #self.osim_model.joints[1].getCoordinate(0).setValue(self.osim_model.state, 0.2195816)\n",
    "        self.osim_model.joints[0].getCoordinate(0).setValue(self.osim_model.state, 0.0)\n",
    "        self.osim_model.joints[1].getCoordinate(0).setValue(self.osim_model.state, 0.0)\n",
    "        #muscle.setActivation(self.osim_model.state, float(action[0]))\n",
    "        print(self.osim_model.joints[0].getCoordinate(0).getValue(self.osim_model.state))\n",
    "\n",
    "    def reset(self):\n",
    "        self.new_target()\n",
    "        return super(ArmEnv, self).reset()\n",
    "\n",
    "    def compute_reward(self):\n",
    "        obs = self.get_observation()\n",
    "        #pos = (self.angular_dist(obs[2],self.shoulder) + self.angular_dist(obs[3],self.elbow))\n",
    "        #speed = 0 #(obs[4]**2 + obs[5]**2) / 200.0\n",
    "        \"\"\"\n",
    "        if -0.1 < obs[2] and obs[2] < 0.1 and -0.1 < obs[3] and obs[3] < 0.1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        #return - pos - speed\n",
    "        #return 1.0\n",
    "        \"\"\"\n",
    "        #costs = (obs[2]+0.3)**2 + (obs[3]-0.3)**2 + 0.1*(obs[4])**2 + (obs[5])**2\n",
    "        costs = (obs[2]-0.4*np.cos(0.001*self.istep))**2 + (obs[3]-0.4*np.sin(0.001*self.istep))**2\n",
    "        return -costs\n",
    "\n",
    "    def get_observation(self):\n",
    "        invars = np.array([0] * self.ninput, dtype='f')\n",
    "        \n",
    "        \n",
    "\n",
    "        invars[0] = self.shoulder\n",
    "        invars[1] = self.elbow\n",
    "        \n",
    "        invars[2] = self.osim_model.joints[0].getCoordinate(0).getValue(self.osim_model.state)\n",
    "        invars[3] = self.osim_model.joints[1].getCoordinate(0).getValue(self.osim_model.state)\n",
    "\n",
    "        invars[4] = self.osim_model.joints[0].getCoordinate(0).getSpeedValue(self.osim_model.state)\n",
    "        invars[5] = self.osim_model.joints[1].getCoordinate(0).getSpeedValue(self.osim_model.state)\n",
    "\n",
    "        invars[6] = self.sanitify(self.osim_model.joints[0].getCoordinate(0).getAccelerationValue(self.osim_model.state))\n",
    "        invars[7] = self.sanitify(self.osim_model.joints[1].getCoordinate(0).getAccelerationValue(self.osim_model.state))\n",
    "\n",
    "        pos = self.osim_model.model.calcMassCenterPosition(self.osim_model.state)\n",
    "        vel = self.osim_model.model.calcMassCenterVelocity(self.osim_model.state)\n",
    "        \n",
    "        invars[8] = pos[0]\n",
    "        invars[9] = pos[1]\n",
    "        invars[10] = pos[2]\n",
    "\n",
    "        invars[11] = vel[0]\n",
    "        invars[12] = vel[1]\n",
    "        invars[13] = vel[2]\n",
    "\n",
    "        return invars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import opensim\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from osim.env import OsimEnv\n",
    "\n",
    "class ArmEnv(OsimEnv):\n",
    "    ninput = 14\n",
    "    model_path = './models/arm2dof6musc.osim'\n",
    "\n",
    "    def __init__(self, visualize = False):\n",
    "        self.iepisode = 0\n",
    "        self.shoulder = 0.0\n",
    "        self.elbow = 0.0\n",
    "        super(ArmEnv, self).__init__(visualize = visualize)\n",
    "\n",
    "    def configure(self):\n",
    "        super(ArmEnv, self).configure()\n",
    "        self.osim_model.joints.append(opensim.CustomJoint.safeDownCast(self.osim_model.jointSet.get(0)))\n",
    "        self.osim_model.joints.append(opensim.CustomJoint.safeDownCast(self.osim_model.jointSet.get(1)))\n",
    "\n",
    "    def new_target(self):\n",
    "        self.shoulder = random.uniform(-1.2,0.3)\n",
    "        self.elbow = random.uniform(-1.0,0)\n",
    "\n",
    "    def reset(self):\n",
    "        self.new_target()\n",
    "        return super(ArmEnv, self).reset()\n",
    "\n",
    "    def compute_reward(self):\n",
    "        obs = self.get_observation()\n",
    "        pos = (self.angular_dist(obs[2],self.shoulder) + self.angular_dist(obs[3],self.elbow))\n",
    "        speed = 0 #(obs[4]**2 + obs[5]**2) / 200.0\n",
    "        return - pos - speed\n",
    "\n",
    "    def get_observation(self):\n",
    "        invars = np.array([0] * self.ninput, dtype='f')\n",
    "\n",
    "        invars[0] = self.shoulder\n",
    "        invars[1] = self.elbow\n",
    "        \n",
    "        invars[2] = self.osim_model.joints[0].getCoordinate(0).getValue(self.osim_model.state)\n",
    "        invars[3] = self.osim_model.joints[1].getCoordinate(0).getValue(self.osim_model.state)\n",
    "\n",
    "        invars[4] = self.osim_model.joints[0].getCoordinate(0).getSpeedValue(self.osim_model.state)\n",
    "        invars[5] = self.osim_model.joints[1].getCoordinate(0).getSpeedValue(self.osim_model.state)\n",
    "\n",
    "        invars[6] = self.sanitify(self.osim_model.joints[0].getCoordinate(0).getAccelerationValue(self.osim_model.state))\n",
    "        invars[7] = self.sanitify(self.osim_model.joints[1].getCoordinate(0).getAccelerationValue(self.osim_model.state))\n",
    "\n",
    "        pos = self.osim_model.model.calcMassCenterPosition(self.osim_model.state)\n",
    "        vel = self.osim_model.model.calcMassCenterVelocity(self.osim_model.state)\n",
    "        \n",
    "        invars[8] = pos[0]\n",
    "        invars[9] = pos[1]\n",
    "        invars[10] = pos[2]\n",
    "\n",
    "        invars[11] = vel[0]\n",
    "        invars[12] = vel[1]\n",
    "        invars[13] = vel[2]\n",
    "\n",
    "        return invars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments...\n",
      "Namespace(model='example_arm.h5f', steps=100000, train=True, visualize=False)\n",
      "Environment generated\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#%% \n",
    "# Command line parameters\n",
    "#parser = argparse.ArgumentParser(description='Train or test neural net motor controller')\n",
    "#parser.add_argument('--train', dest='train', action='store_true', default=True)\n",
    "#parser.add_argument('--test', dest='train', action='store_false', default=True)\n",
    "#parser.add_argument('--steps', dest='steps', action='store', default=10000)\n",
    "#parser.add_argument('--visualize', dest='visualize', action='store_true', default=False)\n",
    "#parser.add_argument('--model', dest='model', action='store', default=\"example.h5f\")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "\n",
    "print(\"Arguments...\")\n",
    "#print(args)\n",
    "\n",
    "args = argparse.Namespace(model='example_arm.h5f', steps=100000, visualize=False, train=True)\n",
    "print(args)\n",
    "# Load walking environment\n",
    "print(\"Environment generated\")\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('observation'))\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# Load walking environment\n",
    "env = ArmEnv(args.visualize)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "#env.new_new()\n",
    "print(env.timestep_limit)\n",
    "env.timestep_limit = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_1 (Flatten)              (None, 14)            0           flatten_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 32)            480         flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            1056        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 32)            1056        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 6)             198         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 6)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2790\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 14)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 14)            0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 20)            0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 64)            1344        merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 64)            0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 64)            4160        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 64)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 64)            4160        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 64)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             65          activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 1)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 9729\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Training for 100000 steps ...\n",
      "   500/100000: episode: 1, duration: 22.700s, episode steps: 500, steps per second: 22, episode reward: -954.184, mean reward: -1.908 [-6.783, -0.137], mean action: 0.279 [-0.766, 1.246], mean observation: -0.122 [-1000.000, 1000.000], loss: 1.413206, mean_absolute_error: 1.597085, mean_q: -2.302956\n",
      "  1000/100000: episode: 2, duration: 25.570s, episode steps: 500, steps per second: 20, episode reward: -1932.929, mean reward: -3.866 [-15.839, -0.587], mean action: 0.651 [-0.338, 1.493], mean observation: 0.707 [-838.549, 1000.000], loss: 0.919247, mean_absolute_error: 1.143233, mean_q: -4.332006\n",
      "  1500/100000: episode: 3, duration: 24.744s, episode steps: 500, steps per second: 20, episode reward: -2971.209, mean reward: -5.942 [-13.256, -0.164], mean action: 0.623 [-0.363, 1.499], mean observation: -0.547 [-890.508, 473.404], loss: 0.503717, mean_absolute_error: 0.690826, mean_q: -6.030205\n",
      "  2000/100000: episode: 4, duration: 14.967s, episode steps: 500, steps per second: 33, episode reward: -484.314, mean reward: -0.969 [-3.795, -0.053], mean action: 0.270 [-0.450, 1.239], mean observation: -0.113 [-1000.000, 662.012], loss: 0.511947, mean_absolute_error: 0.717862, mean_q: -7.438675\n",
      "  2500/100000: episode: 5, duration: 10.908s, episode steps: 500, steps per second: 46, episode reward: -519.177, mean reward: -1.038 [-3.798, -0.085], mean action: 0.437 [-0.699, 1.422], mean observation: -0.070 [-15.724, 15.933], loss: 0.409176, mean_absolute_error: 0.609855, mean_q: -7.261435\n",
      "  3000/100000: episode: 6, duration: 10.417s, episode steps: 500, steps per second: 48, episode reward: -365.851, mean reward: -0.732 [-3.780, -0.020], mean action: 0.438 [-0.433, 1.232], mean observation: -0.037 [-16.039, 13.471], loss: 0.329952, mean_absolute_error: 0.507687, mean_q: -7.246993\n",
      "  3500/100000: episode: 7, duration: 34.508s, episode steps: 500, steps per second: 14, episode reward: -2040.556, mean reward: -4.081 [-10.606, -0.046], mean action: 0.297 [-0.641, 1.547], mean observation: 0.072 [-273.207, 283.420], loss: 0.251128, mean_absolute_error: 0.425246, mean_q: -7.431995\n",
      "  4000/100000: episode: 8, duration: 11.515s, episode steps: 500, steps per second: 43, episode reward: -2386.534, mean reward: -4.773 [-7.950, -0.005], mean action: 0.085 [-0.556, 1.167], mean observation: 0.137 [-13.569, 7.554], loss: 0.256999, mean_absolute_error: 0.435374, mean_q: -8.367310\n",
      "  4500/100000: episode: 9, duration: 10.396s, episode steps: 500, steps per second: 48, episode reward: -809.541, mean reward: -1.619 [-7.389, -0.016], mean action: 0.202 [-0.446, 1.616], mean observation: -0.006 [-681.601, 453.784], loss: 0.254257, mean_absolute_error: 0.430830, mean_q: -9.327800\n",
      "  5000/100000: episode: 10, duration: 21.207s, episode steps: 500, steps per second: 24, episode reward: -1522.952, mean reward: -3.046 [-15.877, -0.003], mean action: 0.044 [-0.574, 1.142], mean observation: 0.059 [-349.255, 300.319], loss: 0.265618, mean_absolute_error: 0.445865, mean_q: -9.910721\n",
      "  5500/100000: episode: 11, duration: 15.337s, episode steps: 500, steps per second: 33, episode reward: -604.733, mean reward: -1.209 [-3.873, -0.060], mean action: 0.247 [-0.662, 1.350], mean observation: -0.058 [-178.052, 208.256], loss: 0.261556, mean_absolute_error: 0.447757, mean_q: -10.805785\n",
      "  6000/100000: episode: 12, duration: 11.399s, episode steps: 500, steps per second: 44, episode reward: -1686.578, mean reward: -3.373 [-13.693, -0.031], mean action: 0.215 [-0.181, 1.107], mean observation: 0.141 [-9.514, 10.035], loss: 0.255255, mean_absolute_error: 0.440168, mean_q: -11.436453\n",
      "  6500/100000: episode: 13, duration: 9.214s, episode steps: 500, steps per second: 54, episode reward: -416.875, mean reward: -0.834 [-3.873, -0.017], mean action: 0.021 [-0.699, 1.183], mean observation: 0.067 [-11.336, 11.060], loss: 0.275853, mean_absolute_error: 0.459406, mean_q: -12.398426\n",
      "  7000/100000: episode: 14, duration: 27.391s, episode steps: 500, steps per second: 18, episode reward: -563.217, mean reward: -1.126 [-4.310, -0.020], mean action: 0.324 [-0.569, 1.757], mean observation: -1.223 [-1000.000, 1000.000], loss: 0.401688, mean_absolute_error: 0.590991, mean_q: -13.056701\n",
      "  7500/100000: episode: 15, duration: 34.547s, episode steps: 500, steps per second: 14, episode reward: -921.246, mean reward: -1.842 [-7.375, -0.010], mean action: 0.295 [-0.441, 1.513], mean observation: -0.554 [-1000.000, 1000.000], loss: 0.477002, mean_absolute_error: 0.683405, mean_q: -13.554979\n",
      "  8000/100000: episode: 16, duration: 22.905s, episode steps: 500, steps per second: 22, episode reward: -532.391, mean reward: -1.065 [-4.270, -0.116], mean action: 0.433 [-0.418, 1.370], mean observation: 0.059 [-111.953, 761.420], loss: 0.440230, mean_absolute_error: 0.645175, mean_q: -14.080076\n",
      "  8500/100000: episode: 17, duration: 16.366s, episode steps: 500, steps per second: 31, episode reward: -391.068, mean reward: -0.782 [-3.958, -0.000], mean action: 0.308 [-0.523, 1.555], mean observation: 0.138 [-141.951, 968.330], loss: 0.418825, mean_absolute_error: 0.627279, mean_q: -14.293799\n",
      "  9000/100000: episode: 18, duration: 37.847s, episode steps: 500, steps per second: 13, episode reward: -876.272, mean reward: -1.753 [-13.216, -0.004], mean action: 0.214 [-0.572, 1.341], mean observation: 0.106 [-280.189, 281.348], loss: 0.472003, mean_absolute_error: 0.682839, mean_q: -14.528763\n",
      "  9500/100000: episode: 19, duration: 16.400s, episode steps: 500, steps per second: 30, episode reward: -7447.594, mean reward: -14.895 [-37.661, -0.004], mean action: 0.118 [-0.396, 1.251], mean observation: 0.270 [-11.477, 10.672], loss: 0.486649, mean_absolute_error: 0.713842, mean_q: -16.435373\n",
      " 10000/100000: episode: 20, duration: 23.564s, episode steps: 500, steps per second: 21, episode reward: -2967.914, mean reward: -5.936 [-11.891, -0.004], mean action: 0.425 [-0.531, 1.537], mean observation: 0.178 [-9.910, 11.909], loss: 0.471388, mean_absolute_error: 0.711324, mean_q: -19.507948\n",
      " 10500/100000: episode: 21, duration: 9.491s, episode steps: 500, steps per second: 53, episode reward: -3955.522, mean reward: -7.911 [-17.173, -2.570], mean action: 0.454 [-0.538, 1.725], mean observation: 0.313 [-16.433, 595.576], loss: 0.490978, mean_absolute_error: 0.730228, mean_q: -22.089123\n",
      " 11000/100000: episode: 22, duration: 15.366s, episode steps: 500, steps per second: 33, episode reward: -776.940, mean reward: -1.554 [-4.259, -0.207], mean action: 0.475 [-0.258, 1.526], mean observation: 0.136 [-17.212, 285.233], loss: 0.475443, mean_absolute_error: 0.721782, mean_q: -24.060915\n",
      " 11500/100000: episode: 23, duration: 28.155s, episode steps: 500, steps per second: 18, episode reward: -2077.180, mean reward: -4.154 [-11.776, -0.008], mean action: 0.098 [-0.722, 1.316], mean observation: -0.962 [-1000.000, 1000.000], loss: 0.515616, mean_absolute_error: 0.763645, mean_q: -25.599789\n",
      " 12000/100000: episode: 24, duration: 17.333s, episode steps: 500, steps per second: 29, episode reward: -586.265, mean reward: -1.173 [-3.883, -0.066], mean action: 0.357 [-0.536, 1.586], mean observation: -0.528 [-1000.000, 1000.000], loss: 0.671742, mean_absolute_error: 0.927840, mean_q: -26.240164\n",
      " 12500/100000: episode: 25, duration: 15.181s, episode steps: 500, steps per second: 33, episode reward: -518.222, mean reward: -1.036 [-3.874, -0.034], mean action: 0.506 [-0.483, 1.576], mean observation: -0.110 [-1000.000, 1000.000], loss: 0.741173, mean_absolute_error: 1.003286, mean_q: -26.504015\n",
      " 13000/100000: episode: 26, duration: 10.154s, episode steps: 500, steps per second: 49, episode reward: -320.226, mean reward: -0.640 [-3.886, -0.038], mean action: 0.686 [-0.388, 1.658], mean observation: -0.041 [-17.823, 11.684], loss: 0.730884, mean_absolute_error: 0.994277, mean_q: -26.448433\n",
      " 13500/100000: episode: 27, duration: 11.033s, episode steps: 500, steps per second: 45, episode reward: -250.436, mean reward: -0.501 [-3.885, -0.000], mean action: 0.368 [-0.555, 1.169], mean observation: -0.032 [-291.916, 184.725], loss: 0.765127, mean_absolute_error: 1.029247, mean_q: -26.760937\n",
      " 14000/100000: episode: 28, duration: 11.131s, episode steps: 500, steps per second: 45, episode reward: -240.871, mean reward: -0.482 [-3.888, -0.000], mean action: 0.382 [-0.731, 1.518], mean observation: 0.010 [-230.723, 124.634], loss: 0.755215, mean_absolute_error: 1.016484, mean_q: -27.061033\n",
      " 14500/100000: episode: 29, duration: 10.191s, episode steps: 500, steps per second: 49, episode reward: -202.962, mean reward: -0.406 [-3.881, -0.001], mean action: 0.550 [-0.412, 1.614], mean observation: -0.007 [-39.423, 76.096], loss: 0.835005, mean_absolute_error: 1.099556, mean_q: -27.875586\n",
      " 15000/100000: episode: 30, duration: 9.531s, episode steps: 500, steps per second: 52, episode reward: -249.434, mean reward: -0.499 [-3.877, -0.002], mean action: 0.418 [-0.641, 1.358], mean observation: -0.014 [-17.232, 11.568], loss: 0.778567, mean_absolute_error: 1.041374, mean_q: -27.065687\n",
      " 15500/100000: episode: 31, duration: 9.622s, episode steps: 500, steps per second: 52, episode reward: -252.780, mean reward: -0.506 [-3.880, -0.038], mean action: 0.313 [-0.732, 1.298], mean observation: 0.020 [-441.261, 325.045], loss: 0.771072, mean_absolute_error: 1.033052, mean_q: -26.943195\n",
      " 16000/100000: episode: 32, duration: 10.040s, episode steps: 500, steps per second: 50, episode reward: -237.963, mean reward: -0.476 [-3.880, -0.005], mean action: 0.513 [-0.533, 1.348], mean observation: -0.023 [-14.120, 7.855], loss: 0.789461, mean_absolute_error: 1.042372, mean_q: -27.563234\n",
      " 16500/100000: episode: 33, duration: 10.568s, episode steps: 500, steps per second: 47, episode reward: -252.125, mean reward: -0.504 [-3.875, -0.005], mean action: 0.493 [-0.345, 1.610], mean observation: 0.019 [-14.650, 207.557], loss: 0.763122, mean_absolute_error: 1.022631, mean_q: -27.931299\n",
      " 17000/100000: episode: 34, duration: 9.892s, episode steps: 500, steps per second: 51, episode reward: -186.649, mean reward: -0.373 [-3.879, -0.003], mean action: 0.428 [-0.354, 1.393], mean observation: -0.004 [-13.582, 12.099], loss: 0.770024, mean_absolute_error: 1.027321, mean_q: -28.231110\n",
      " 17500/100000: episode: 35, duration: 9.968s, episode steps: 500, steps per second: 50, episode reward: -222.532, mean reward: -0.445 [-3.879, -0.040], mean action: 0.484 [-0.303, 1.589], mean observation: 0.018 [-13.606, 125.425], loss: 0.763191, mean_absolute_error: 1.013762, mean_q: -26.964687\n",
      " 18000/100000: episode: 36, duration: 9.924s, episode steps: 500, steps per second: 50, episode reward: -196.724, mean reward: -0.393 [-3.877, -0.012], mean action: 0.333 [-0.617, 1.321], mean observation: -0.014 [-391.475, 271.569], loss: 0.756374, mean_absolute_error: 1.003808, mean_q: -27.604012\n",
      " 18500/100000: episode: 37, duration: 10.607s, episode steps: 500, steps per second: 47, episode reward: -207.946, mean reward: -0.416 [-3.878, -0.002], mean action: 0.339 [-0.534, 1.492], mean observation: 0.023 [-13.493, 187.003], loss: 0.707961, mean_absolute_error: 0.955909, mean_q: -26.818470\n",
      " 19000/100000: episode: 38, duration: 8.502s, episode steps: 500, steps per second: 59, episode reward: -295.690, mean reward: -0.591 [-3.869, -0.036], mean action: 0.360 [-0.520, 1.269], mean observation: 0.005 [-227.874, 356.729], loss: 0.821611, mean_absolute_error: 1.067036, mean_q: -27.862064\n",
      " 19500/100000: episode: 39, duration: 9.557s, episode steps: 500, steps per second: 52, episode reward: -217.884, mean reward: -0.436 [-3.870, -0.010], mean action: 0.435 [-0.223, 1.248], mean observation: 0.008 [-262.095, 312.152], loss: 0.768692, mean_absolute_error: 1.021646, mean_q: -27.243767\n",
      " 20000/100000: episode: 40, duration: 9.071s, episode steps: 500, steps per second: 55, episode reward: -254.310, mean reward: -0.509 [-3.876, -0.074], mean action: 0.478 [-0.565, 1.305], mean observation: -0.039 [-1000.000, 880.201], loss: 0.741147, mean_absolute_error: 0.985739, mean_q: -27.545347\n",
      " 20500/100000: episode: 41, duration: 10.917s, episode steps: 500, steps per second: 46, episode reward: -273.878, mean reward: -0.548 [-3.872, -0.002], mean action: 0.542 [-0.299, 1.468], mean observation: -0.018 [-162.141, 196.397], loss: 0.738595, mean_absolute_error: 0.977191, mean_q: -26.910686\n",
      " 21000/100000: episode: 42, duration: 9.714s, episode steps: 500, steps per second: 51, episode reward: -351.206, mean reward: -0.702 [-3.870, -0.001], mean action: 0.374 [-0.577, 1.303], mean observation: -0.014 [-12.516, 10.727], loss: 0.708747, mean_absolute_error: 0.948873, mean_q: -26.983486\n",
      " 21500/100000: episode: 43, duration: 10.042s, episode steps: 500, steps per second: 50, episode reward: -184.693, mean reward: -0.369 [-3.875, -0.007], mean action: 0.341 [-0.542, 1.821], mean observation: -0.000 [-13.554, 13.134], loss: 0.688016, mean_absolute_error: 0.926499, mean_q: -26.374746\n",
      " 22000/100000: episode: 44, duration: 10.275s, episode steps: 500, steps per second: 49, episode reward: -219.099, mean reward: -0.438 [-3.872, -0.036], mean action: 0.411 [-0.396, 1.577], mean observation: -0.021 [-1000.000, 854.151], loss: 0.764444, mean_absolute_error: 1.001221, mean_q: -26.480589\n",
      " 22500/100000: episode: 45, duration: 10.301s, episode steps: 500, steps per second: 49, episode reward: -205.955, mean reward: -0.412 [-3.873, -0.010], mean action: 0.385 [-0.393, 1.493], mean observation: 0.032 [-11.372, 215.574], loss: 0.701500, mean_absolute_error: 0.942024, mean_q: -26.536440\n",
      " 23000/100000: episode: 46, duration: 8.241s, episode steps: 500, steps per second: 61, episode reward: -235.777, mean reward: -0.472 [-3.872, -0.001], mean action: 0.481 [-0.348, 1.559], mean observation: -0.035 [-1000.000, 677.038], loss: 0.829920, mean_absolute_error: 1.066848, mean_q: -27.587154\n",
      " 23500/100000: episode: 47, duration: 9.463s, episode steps: 500, steps per second: 53, episode reward: -209.288, mean reward: -0.419 [-3.870, -0.000], mean action: 0.656 [-0.230, 1.775], mean observation: 0.022 [-112.860, 262.712], loss: 0.800461, mean_absolute_error: 1.035490, mean_q: -26.781107\n",
      " 24000/100000: episode: 48, duration: 10.331s, episode steps: 500, steps per second: 48, episode reward: -180.243, mean reward: -0.360 [-3.875, -0.036], mean action: 0.549 [-0.688, 1.642], mean observation: 0.027 [-10.968, 205.489], loss: 0.660969, mean_absolute_error: 0.896915, mean_q: -26.627407\n",
      " 24500/100000: episode: 49, duration: 9.501s, episode steps: 500, steps per second: 53, episode reward: -198.990, mean reward: -0.398 [-3.873, -0.054], mean action: 0.515 [-0.445, 1.396], mean observation: 0.043 [-94.317, 227.533], loss: 0.711346, mean_absolute_error: 0.939225, mean_q: -26.486422\n",
      " 25000/100000: episode: 50, duration: 10.039s, episode steps: 500, steps per second: 50, episode reward: -186.321, mean reward: -0.373 [-3.875, -0.007], mean action: 0.502 [-0.831, 1.782], mean observation: 0.027 [-902.319, 483.887], loss: 0.832168, mean_absolute_error: 1.052207, mean_q: -25.851749\n",
      " 25500/100000: episode: 51, duration: 11.654s, episode steps: 500, steps per second: 43, episode reward: -204.343, mean reward: -0.409 [-3.875, -0.034], mean action: 0.463 [-0.807, 1.928], mean observation: 0.021 [-12.963, 10.163], loss: 0.647983, mean_absolute_error: 0.871792, mean_q: -25.683813\n",
      " 26000/100000: episode: 52, duration: 9.032s, episode steps: 500, steps per second: 55, episode reward: -192.091, mean reward: -0.384 [-3.875, -0.018], mean action: 0.358 [-0.635, 1.534], mean observation: -0.005 [-11.578, 9.801], loss: 0.798478, mean_absolute_error: 1.024608, mean_q: -26.681210\n",
      " 26500/100000: episode: 53, duration: 7.670s, episode steps: 500, steps per second: 65, episode reward: -222.415, mean reward: -0.445 [-3.878, -0.042], mean action: 0.541 [-0.383, 1.307], mean observation: -0.013 [-12.195, 10.442], loss: 0.678745, mean_absolute_error: 0.901125, mean_q: -26.431465\n",
      " 27000/100000: episode: 54, duration: 8.513s, episode steps: 500, steps per second: 59, episode reward: -211.761, mean reward: -0.424 [-3.877, -0.020], mean action: 0.494 [-0.353, 1.407], mean observation: -0.114 [-1000.000, 654.489], loss: 0.777984, mean_absolute_error: 1.005961, mean_q: -25.899366\n",
      " 27500/100000: episode: 55, duration: 9.226s, episode steps: 500, steps per second: 54, episode reward: -174.161, mean reward: -0.348 [-3.878, -0.056], mean action: 0.456 [-0.255, 1.797], mean observation: 0.037 [-11.757, 162.194], loss: 0.701923, mean_absolute_error: 0.926290, mean_q: -25.793320\n",
      " 28000/100000: episode: 56, duration: 9.970s, episode steps: 500, steps per second: 50, episode reward: -201.009, mean reward: -0.402 [-3.875, -0.024], mean action: 0.581 [-0.164, 1.648], mean observation: -0.003 [-13.912, 11.376], loss: 0.665053, mean_absolute_error: 0.883722, mean_q: -25.201231\n",
      " 28500/100000: episode: 57, duration: 8.188s, episode steps: 500, steps per second: 61, episode reward: -202.254, mean reward: -0.405 [-3.876, -0.006], mean action: 0.414 [-0.499, 1.454], mean observation: -0.008 [-10.811, 8.806], loss: 0.753441, mean_absolute_error: 0.970328, mean_q: -25.334326\n",
      " 29000/100000: episode: 58, duration: 8.022s, episode steps: 500, steps per second: 62, episode reward: -178.074, mean reward: -0.356 [-3.876, -0.044], mean action: 0.501 [-0.384, 1.522], mean observation: -0.166 [-1000.000, 941.522], loss: 0.675328, mean_absolute_error: 0.893291, mean_q: -25.016260\n",
      " 29500/100000: episode: 59, duration: 9.309s, episode steps: 500, steps per second: 54, episode reward: -178.705, mean reward: -0.357 [-3.877, -0.024], mean action: 0.498 [-0.457, 1.389], mean observation: 0.001 [-14.286, 14.910], loss: 0.660095, mean_absolute_error: 0.878968, mean_q: -24.678265\n",
      " 30000/100000: episode: 60, duration: 9.178s, episode steps: 500, steps per second: 54, episode reward: -184.093, mean reward: -0.368 [-3.876, -0.012], mean action: 0.393 [-0.230, 1.155], mean observation: -0.028 [-791.036, 472.421], loss: 0.663185, mean_absolute_error: 0.873867, mean_q: -24.512531\n",
      " 30500/100000: episode: 61, duration: 8.127s, episode steps: 500, steps per second: 62, episode reward: -189.745, mean reward: -0.379 [-3.877, -0.034], mean action: 0.477 [-0.496, 1.983], mean observation: 0.036 [-441.072, 316.296], loss: 0.762678, mean_absolute_error: 0.974064, mean_q: -24.185125\n",
      " 31000/100000: episode: 62, duration: 9.313s, episode steps: 500, steps per second: 54, episode reward: -210.174, mean reward: -0.420 [-3.876, -0.004], mean action: 0.366 [-0.343, 1.090], mean observation: 0.044 [-65.517, 232.726], loss: 0.640047, mean_absolute_error: 0.850637, mean_q: -24.803238\n",
      " 31500/100000: episode: 63, duration: 8.007s, episode steps: 500, steps per second: 62, episode reward: -163.197, mean reward: -0.326 [-3.877, -0.059], mean action: 0.345 [-0.591, 1.618], mean observation: -0.001 [-12.204, 15.157], loss: 0.737781, mean_absolute_error: 0.944746, mean_q: -24.893227\n",
      " 32000/100000: episode: 64, duration: 11.294s, episode steps: 500, steps per second: 44, episode reward: -189.673, mean reward: -0.379 [-3.877, -0.018], mean action: 0.522 [-0.467, 1.838], mean observation: -0.011 [-1000.000, 1000.000], loss: 0.629249, mean_absolute_error: 0.834233, mean_q: -24.372461\n",
      " 32500/100000: episode: 65, duration: 10.993s, episode steps: 500, steps per second: 45, episode reward: -174.340, mean reward: -0.349 [-3.872, -0.044], mean action: 0.441 [-0.345, 1.481], mean observation: 0.012 [-90.440, 173.281], loss: 0.722118, mean_absolute_error: 0.927221, mean_q: -24.117229\n",
      " 33000/100000: episode: 66, duration: 9.617s, episode steps: 500, steps per second: 52, episode reward: -169.972, mean reward: -0.340 [-3.865, -0.024], mean action: 0.435 [-0.459, 1.457], mean observation: 0.003 [-12.563, 16.663], loss: 0.645805, mean_absolute_error: 0.848612, mean_q: -23.841375\n",
      " 33500/100000: episode: 67, duration: 9.159s, episode steps: 500, steps per second: 55, episode reward: -192.768, mean reward: -0.386 [-3.863, -0.086], mean action: 0.573 [-0.490, 1.516], mean observation: 0.015 [-31.879, 224.350], loss: 0.707304, mean_absolute_error: 0.911246, mean_q: -23.976934\n",
      " 34000/100000: episode: 68, duration: 11.727s, episode steps: 500, steps per second: 43, episode reward: -180.374, mean reward: -0.361 [-3.863, -0.087], mean action: 0.409 [-0.548, 1.784], mean observation: -0.003 [-1000.000, 1000.000], loss: 0.680669, mean_absolute_error: 0.881827, mean_q: -23.528297\n",
      " 34500/100000: episode: 69, duration: 9.543s, episode steps: 500, steps per second: 52, episode reward: -211.608, mean reward: -0.423 [-3.863, -0.027], mean action: 0.490 [-0.408, 1.391], mean observation: -0.014 [-12.477, 10.114], loss: 0.620930, mean_absolute_error: 0.825941, mean_q: -24.125538\n",
      " 35000/100000: episode: 70, duration: 8.958s, episode steps: 500, steps per second: 56, episode reward: -186.779, mean reward: -0.374 [-3.863, -0.048], mean action: 0.534 [-0.696, 1.741], mean observation: 0.021 [-30.678, 221.979], loss: 0.671097, mean_absolute_error: 0.867846, mean_q: -23.500477\n",
      " 35500/100000: episode: 71, duration: 9.918s, episode steps: 500, steps per second: 50, episode reward: -164.827, mean reward: -0.330 [-3.863, -0.050], mean action: 0.569 [-0.271, 1.615], mean observation: 0.046 [-51.253, 339.679], loss: 0.660698, mean_absolute_error: 0.855641, mean_q: -23.355148\n",
      " 36000/100000: episode: 72, duration: 7.864s, episode steps: 500, steps per second: 64, episode reward: -198.996, mean reward: -0.398 [-3.863, -0.043], mean action: 0.501 [-0.319, 1.442], mean observation: 0.024 [-42.484, 284.553], loss: 0.689691, mean_absolute_error: 0.889491, mean_q: -23.115307\n",
      " 36500/100000: episode: 73, duration: 7.741s, episode steps: 500, steps per second: 65, episode reward: -174.429, mean reward: -0.349 [-3.862, -0.066], mean action: 0.555 [-0.415, 1.836], mean observation: 0.009 [-1000.000, 1000.000], loss: 0.601867, mean_absolute_error: 0.797955, mean_q: -22.752047\n",
      " 37000/100000: episode: 74, duration: 10.506s, episode steps: 500, steps per second: 48, episode reward: -180.288, mean reward: -0.361 [-3.863, -0.031], mean action: 0.407 [-0.436, 1.372], mean observation: 0.035 [-43.696, 286.966], loss: 0.680522, mean_absolute_error: 0.878209, mean_q: -23.425915\n",
      " 37500/100000: episode: 75, duration: 8.569s, episode steps: 500, steps per second: 58, episode reward: -158.559, mean reward: -0.317 [-3.863, -0.052], mean action: 0.474 [-0.416, 1.454], mean observation: 0.048 [-49.210, 338.161], loss: 0.746056, mean_absolute_error: 0.938981, mean_q: -23.092957\n",
      " 38000/100000: episode: 76, duration: 9.739s, episode steps: 500, steps per second: 51, episode reward: -161.016, mean reward: -0.322 [-3.862, -0.079], mean action: 0.334 [-0.539, 1.493], mean observation: 0.018 [-11.386, 17.162], loss: 0.640631, mean_absolute_error: 0.828153, mean_q: -21.884157\n",
      " 38500/100000: episode: 77, duration: 8.367s, episode steps: 500, steps per second: 60, episode reward: -181.948, mean reward: -0.364 [-3.862, -0.060], mean action: 0.419 [-0.997, 1.329], mean observation: -0.005 [-1000.000, 1000.000], loss: 0.643395, mean_absolute_error: 0.834453, mean_q: -23.573141\n",
      " 39000/100000: episode: 78, duration: 10.441s, episode steps: 500, steps per second: 48, episode reward: -171.921, mean reward: -0.344 [-3.864, -0.023], mean action: 0.461 [-0.728, 1.528], mean observation: 0.053 [-22.693, 327.899], loss: 0.631732, mean_absolute_error: 0.821283, mean_q: -22.487480\n",
      " 39500/100000: episode: 79, duration: 9.309s, episode steps: 500, steps per second: 54, episode reward: -194.561, mean reward: -0.389 [-3.862, -0.039], mean action: 0.490 [-0.230, 1.265], mean observation: -0.010 [-13.141, 13.964], loss: 0.570190, mean_absolute_error: 0.760461, mean_q: -22.714258\n",
      " 40000/100000: episode: 80, duration: 13.291s, episode steps: 500, steps per second: 38, episode reward: -184.702, mean reward: -0.369 [-3.863, -0.033], mean action: 0.295 [-0.559, 1.521], mean observation: 1.925 [-1000.000, 1000.000], loss: 0.660412, mean_absolute_error: 0.844980, mean_q: -23.708702\n",
      " 40500/100000: episode: 81, duration: 13.119s, episode steps: 500, steps per second: 38, episode reward: -482.834, mean reward: -0.966 [-4.023, -0.021], mean action: 0.280 [-0.651, 1.475], mean observation: 1.212 [-1000.000, 1000.000], loss: 0.816932, mean_absolute_error: 1.003274, mean_q: -23.604805\n",
      " 41000/100000: episode: 82, duration: 13.173s, episode steps: 500, steps per second: 38, episode reward: -256.173, mean reward: -0.512 [-3.863, -0.012], mean action: 0.404 [-0.607, 1.379], mean observation: 1.370 [-1000.000, 1000.000], loss: 0.885180, mean_absolute_error: 1.072385, mean_q: -22.229816\n",
      " 41500/100000: episode: 83, duration: 14.020s, episode steps: 500, steps per second: 36, episode reward: -274.533, mean reward: -0.549 [-3.862, -0.046], mean action: 0.556 [-0.268, 1.404], mean observation: -0.070 [-1000.000, 1000.000], loss: 0.817490, mean_absolute_error: 1.010562, mean_q: -23.181965\n",
      " 42000/100000: episode: 84, duration: 15.014s, episode steps: 500, steps per second: 33, episode reward: -230.140, mean reward: -0.460 [-3.862, -0.030], mean action: 0.258 [-0.780, 1.511], mean observation: 0.675 [-1000.000, 1000.000], loss: 0.833385, mean_absolute_error: 1.031438, mean_q: -22.568428\n",
      " 42500/100000: episode: 85, duration: 11.967s, episode steps: 500, steps per second: 42, episode reward: -172.015, mean reward: -0.344 [-3.859, -0.007], mean action: 0.359 [-0.530, 1.281], mean observation: 2.114 [-1000.000, 1000.000], loss: 0.771802, mean_absolute_error: 0.970454, mean_q: -22.185602\n",
      " 43000/100000: episode: 86, duration: 11.439s, episode steps: 500, steps per second: 44, episode reward: -243.704, mean reward: -0.487 [-3.860, -0.004], mean action: 0.577 [-0.657, 1.366], mean observation: 1.452 [-1000.000, 1000.000], loss: 0.852169, mean_absolute_error: 1.056736, mean_q: -22.192028\n",
      " 43500/100000: episode: 87, duration: 13.274s, episode steps: 500, steps per second: 38, episode reward: -206.439, mean reward: -0.413 [-3.862, -0.004], mean action: 0.363 [-0.548, 1.280], mean observation: 2.447 [-1000.000, 1000.000], loss: 0.823769, mean_absolute_error: 1.017353, mean_q: -21.707418\n",
      " 44000/100000: episode: 88, duration: 11.914s, episode steps: 500, steps per second: 42, episode reward: -164.897, mean reward: -0.330 [-3.863, -0.046], mean action: 0.433 [-0.586, 1.313], mean observation: 1.600 [-1000.000, 1000.000], loss: 0.810260, mean_absolute_error: 1.011533, mean_q: -23.120750\n",
      " 44500/100000: episode: 89, duration: 11.845s, episode steps: 500, steps per second: 42, episode reward: -211.914, mean reward: -0.424 [-3.863, -0.056], mean action: 0.599 [-0.459, 1.509], mean observation: 0.379 [-1000.000, 1000.000], loss: 0.813523, mean_absolute_error: 1.019596, mean_q: -21.929359\n",
      " 45000/100000: episode: 90, duration: 12.234s, episode steps: 500, steps per second: 41, episode reward: -176.923, mean reward: -0.354 [-3.862, -0.010], mean action: 0.418 [-0.764, 1.561], mean observation: 2.089 [-1000.000, 1000.000], loss: 0.775810, mean_absolute_error: 0.980869, mean_q: -22.456863\n",
      " 45500/100000: episode: 91, duration: 12.772s, episode steps: 500, steps per second: 39, episode reward: -277.482, mean reward: -0.555 [-3.864, -0.030], mean action: 0.163 [-0.853, 1.129], mean observation: 0.721 [-1000.000, 1000.000], loss: 0.857832, mean_absolute_error: 1.069581, mean_q: -21.459364\n",
      " 46000/100000: episode: 92, duration: 12.196s, episode steps: 500, steps per second: 41, episode reward: -164.046, mean reward: -0.328 [-3.867, -0.028], mean action: 0.491 [-0.469, 1.741], mean observation: -0.233 [-1000.000, 1000.000], loss: 0.731540, mean_absolute_error: 0.930940, mean_q: -21.217999\n",
      " 46500/100000: episode: 93, duration: 12.128s, episode steps: 500, steps per second: 41, episode reward: -172.630, mean reward: -0.345 [-3.865, -0.021], mean action: 0.447 [-0.579, 1.664], mean observation: -0.124 [-848.232, 1000.000], loss: 0.765309, mean_absolute_error: 0.971701, mean_q: -21.428997\n",
      " 47000/100000: episode: 94, duration: 8.976s, episode steps: 500, steps per second: 56, episode reward: -165.336, mean reward: -0.331 [-3.862, -0.045], mean action: 0.561 [-0.527, 1.647], mean observation: -0.149 [-1000.000, 1000.000], loss: 0.769466, mean_absolute_error: 0.976133, mean_q: -21.586929\n",
      " 47500/100000: episode: 95, duration: 9.004s, episode steps: 500, steps per second: 56, episode reward: -191.444, mean reward: -0.383 [-3.864, -0.068], mean action: 0.678 [-0.196, 1.911], mean observation: 0.003 [-16.069, 14.550], loss: 0.719474, mean_absolute_error: 0.918731, mean_q: -20.989859\n",
      " 48000/100000: episode: 96, duration: 10.474s, episode steps: 500, steps per second: 48, episode reward: -182.954, mean reward: -0.366 [-3.860, -0.030], mean action: 0.411 [-0.497, 1.488], mean observation: 0.021 [-178.918, 295.995], loss: 0.812999, mean_absolute_error: 1.010755, mean_q: -20.346718\n",
      " 48500/100000: episode: 97, duration: 8.745s, episode steps: 500, steps per second: 57, episode reward: -204.255, mean reward: -0.409 [-3.864, -0.010], mean action: 0.250 [-0.751, 1.390], mean observation: 0.021 [-15.057, 13.064], loss: 0.663812, mean_absolute_error: 0.863134, mean_q: -21.257252\n",
      " 49000/100000: episode: 98, duration: 9.409s, episode steps: 500, steps per second: 53, episode reward: -162.716, mean reward: -0.325 [-3.862, -0.027], mean action: 0.328 [-0.515, 1.370], mean observation: 0.026 [-108.108, 374.523], loss: 0.714674, mean_absolute_error: 0.911612, mean_q: -20.831976\n",
      " 49500/100000: episode: 99, duration: 9.082s, episode steps: 500, steps per second: 55, episode reward: -206.301, mean reward: -0.413 [-3.861, -0.032], mean action: 0.610 [-0.190, 1.527], mean observation: -0.045 [-518.092, 367.978], loss: 0.717182, mean_absolute_error: 0.918365, mean_q: -20.607220\n",
      " 50000/100000: episode: 100, duration: 8.633s, episode steps: 500, steps per second: 58, episode reward: -166.722, mean reward: -0.333 [-3.864, -0.011], mean action: 0.357 [-0.755, 1.694], mean observation: 0.361 [-172.105, 1000.000], loss: 0.772431, mean_absolute_error: 0.971226, mean_q: -21.239422\n",
      " 50500/100000: episode: 101, duration: 7.845s, episode steps: 500, steps per second: 64, episode reward: -312.362, mean reward: -0.625 [-3.857, -0.001], mean action: 0.646 [-0.340, 1.476], mean observation: 0.400 [-1000.000, 1000.000], loss: 0.661489, mean_absolute_error: 0.857674, mean_q: -20.342508\n",
      " 51000/100000: episode: 102, duration: 8.772s, episode steps: 500, steps per second: 57, episode reward: -174.181, mean reward: -0.348 [-3.859, -0.056], mean action: 0.510 [-0.382, 1.411], mean observation: -0.032 [-1000.000, 862.020], loss: 0.682576, mean_absolute_error: 0.876269, mean_q: -20.732687\n",
      " 51500/100000: episode: 103, duration: 8.474s, episode steps: 500, steps per second: 59, episode reward: -199.422, mean reward: -0.399 [-3.862, -0.018], mean action: 0.389 [-0.755, 1.363], mean observation: -0.000 [-34.678, 229.860], loss: 0.732007, mean_absolute_error: 0.924493, mean_q: -20.284252\n",
      " 52000/100000: episode: 104, duration: 9.274s, episode steps: 500, steps per second: 54, episode reward: -222.493, mean reward: -0.445 [-3.861, -0.090], mean action: 0.474 [-0.326, 1.301], mean observation: -0.070 [-929.636, 376.448], loss: 0.588687, mean_absolute_error: 0.779502, mean_q: -21.039696\n",
      " 52500/100000: episode: 105, duration: 8.435s, episode steps: 500, steps per second: 59, episode reward: -193.629, mean reward: -0.387 [-3.860, -0.033], mean action: 0.623 [-0.228, 1.429], mean observation: 0.424 [-1000.000, 1000.000], loss: 0.649017, mean_absolute_error: 0.838740, mean_q: -20.480167\n",
      " 53000/100000: episode: 106, duration: 10.305s, episode steps: 500, steps per second: 49, episode reward: -170.711, mean reward: -0.341 [-3.862, -0.044], mean action: 0.425 [-0.606, 1.469], mean observation: -0.028 [-1000.000, 1000.000], loss: 0.646828, mean_absolute_error: 0.840337, mean_q: -20.056238\n",
      " 53500/100000: episode: 107, duration: 8.539s, episode steps: 500, steps per second: 59, episode reward: -191.545, mean reward: -0.383 [-3.860, -0.016], mean action: 0.468 [-0.330, 1.368], mean observation: 0.219 [-1000.000, 1000.000], loss: 0.684761, mean_absolute_error: 0.877630, mean_q: -20.078375\n",
      " 54000/100000: episode: 108, duration: 9.599s, episode steps: 500, steps per second: 52, episode reward: -165.562, mean reward: -0.331 [-3.862, -0.046], mean action: 0.400 [-0.626, 1.672], mean observation: -0.111 [-1000.000, 910.597], loss: 0.550569, mean_absolute_error: 0.731767, mean_q: -20.386774\n",
      " 54500/100000: episode: 109, duration: 9.825s, episode steps: 500, steps per second: 51, episode reward: -188.856, mean reward: -0.378 [-3.862, -0.098], mean action: 0.423 [-0.301, 1.388], mean observation: 0.375 [-1000.000, 1000.000], loss: 0.624587, mean_absolute_error: 0.808567, mean_q: -20.339016\n",
      " 55000/100000: episode: 110, duration: 8.621s, episode steps: 500, steps per second: 58, episode reward: -205.072, mean reward: -0.410 [-3.862, -0.053], mean action: 0.613 [-0.534, 1.535], mean observation: 0.217 [-1000.000, 1000.000], loss: 0.606772, mean_absolute_error: 0.791445, mean_q: -19.484039\n",
      " 55500/100000: episode: 111, duration: 9.709s, episode steps: 500, steps per second: 51, episode reward: -183.305, mean reward: -0.367 [-3.865, -0.093], mean action: 0.454 [-0.679, 1.397], mean observation: 0.105 [-1000.000, 795.503], loss: 0.607928, mean_absolute_error: 0.793032, mean_q: -20.380846\n",
      " 56000/100000: episode: 112, duration: 8.365s, episode steps: 500, steps per second: 60, episode reward: -177.637, mean reward: -0.355 [-3.863, -0.052], mean action: 0.527 [-0.220, 1.591], mean observation: 0.270 [-1000.000, 1000.000], loss: 0.600429, mean_absolute_error: 0.780311, mean_q: -19.564837\n",
      " 56500/100000: episode: 113, duration: 8.014s, episode steps: 500, steps per second: 62, episode reward: -198.870, mean reward: -0.398 [-3.863, -0.050], mean action: 0.470 [-0.358, 1.430], mean observation: 0.011 [-1000.000, 799.096], loss: 0.600284, mean_absolute_error: 0.783277, mean_q: -19.783529\n",
      " 57000/100000: episode: 114, duration: 9.396s, episode steps: 500, steps per second: 53, episode reward: -176.675, mean reward: -0.353 [-3.877, -0.033], mean action: 0.470 [-0.491, 1.551], mean observation: -0.122 [-1000.000, 709.516], loss: 0.616603, mean_absolute_error: 0.801279, mean_q: -20.284538\n",
      " 57500/100000: episode: 115, duration: 9.808s, episode steps: 500, steps per second: 51, episode reward: -192.754, mean reward: -0.386 [-3.877, -0.001], mean action: 0.422 [-0.553, 1.262], mean observation: 0.102 [-1000.000, 860.466], loss: 0.545352, mean_absolute_error: 0.732490, mean_q: -19.603516\n",
      " 58000/100000: episode: 116, duration: 8.359s, episode steps: 500, steps per second: 60, episode reward: -170.346, mean reward: -0.341 [-3.879, -0.020], mean action: 0.452 [-0.652, 1.464], mean observation: 0.029 [-15.824, 132.659], loss: 0.628498, mean_absolute_error: 0.808677, mean_q: -19.753113\n",
      " 58500/100000: episode: 117, duration: 11.005s, episode steps: 500, steps per second: 45, episode reward: -186.304, mean reward: -0.373 [-3.876, -0.017], mean action: 0.443 [-0.558, 1.541], mean observation: 0.045 [-15.491, 200.507], loss: 0.572189, mean_absolute_error: 0.751023, mean_q: -19.396132\n",
      " 59000/100000: episode: 118, duration: 9.033s, episode steps: 500, steps per second: 55, episode reward: -181.008, mean reward: -0.362 [-3.878, -0.040], mean action: 0.509 [-0.273, 1.696], mean observation: 0.000 [-1000.000, 769.958], loss: 0.585529, mean_absolute_error: 0.765083, mean_q: -19.597723\n",
      " 59500/100000: episode: 119, duration: 9.409s, episode steps: 500, steps per second: 53, episode reward: -188.649, mean reward: -0.377 [-3.877, -0.010], mean action: 0.373 [-0.441, 1.245], mean observation: 0.066 [-1000.000, 956.994], loss: 0.579084, mean_absolute_error: 0.752684, mean_q: -19.670380\n",
      " 60000/100000: episode: 120, duration: 8.637s, episode steps: 500, steps per second: 58, episode reward: -177.303, mean reward: -0.355 [-3.876, -0.009], mean action: 0.363 [-0.605, 1.647], mean observation: 0.046 [-1000.000, 1000.000], loss: 0.569133, mean_absolute_error: 0.757214, mean_q: -19.666838\n",
      " 60500/100000: episode: 121, duration: 9.042s, episode steps: 500, steps per second: 55, episode reward: -184.241, mean reward: -0.368 [-3.879, -0.001], mean action: 0.425 [-0.396, 1.394], mean observation: 0.003 [-16.206, 18.462], loss: 0.544368, mean_absolute_error: 0.722578, mean_q: -19.446054\n",
      " 61000/100000: episode: 122, duration: 7.891s, episode steps: 500, steps per second: 63, episode reward: -182.302, mean reward: -0.365 [-3.877, -0.003], mean action: 0.438 [-0.618, 1.349], mean observation: -0.004 [-1000.000, 1000.000], loss: 0.641807, mean_absolute_error: 0.819825, mean_q: -19.997015\n",
      " 61500/100000: episode: 123, duration: 10.285s, episode steps: 500, steps per second: 49, episode reward: -205.615, mean reward: -0.411 [-3.877, -0.001], mean action: 0.437 [-0.628, 1.502], mean observation: 0.062 [-16.383, 168.321], loss: 0.595008, mean_absolute_error: 0.772687, mean_q: -19.423265\n",
      " 62000/100000: episode: 124, duration: 9.224s, episode steps: 500, steps per second: 54, episode reward: -171.292, mean reward: -0.343 [-3.777, -0.000], mean action: 0.492 [-0.505, 1.579], mean observation: -0.077 [-1000.000, 874.566], loss: 0.526658, mean_absolute_error: 0.706646, mean_q: -19.158012\n",
      " 62500/100000: episode: 125, duration: 14.074s, episode steps: 500, steps per second: 36, episode reward: -2149.680, mean reward: -4.299 [-12.049, -0.046], mean action: 0.386 [-0.460, 1.419], mean observation: -0.122 [-1000.000, 1000.000], loss: 0.517741, mean_absolute_error: 0.698612, mean_q: -18.697823\n",
      " 63000/100000: episode: 126, duration: 9.013s, episode steps: 500, steps per second: 55, episode reward: -173.924, mean reward: -0.348 [-3.777, -0.023], mean action: 0.359 [-1.026, 1.413], mean observation: -0.002 [-1000.000, 1000.000], loss: 0.570427, mean_absolute_error: 0.749208, mean_q: -19.685968\n",
      " 63500/100000: episode: 127, duration: 10.599s, episode steps: 500, steps per second: 47, episode reward: -189.879, mean reward: -0.380 [-3.777, -0.026], mean action: 0.458 [-0.631, 1.819], mean observation: 0.078 [-449.330, 1000.000], loss: 0.631509, mean_absolute_error: 0.813316, mean_q: -19.756809\n",
      " 64000/100000: episode: 128, duration: 7.847s, episode steps: 500, steps per second: 64, episode reward: -172.327, mean reward: -0.345 [-3.779, -0.016], mean action: 0.451 [-0.435, 1.335], mean observation: 0.000 [-173.040, 172.988], loss: 0.551651, mean_absolute_error: 0.729481, mean_q: -19.346872\n",
      " 64500/100000: episode: 129, duration: 7.968s, episode steps: 500, steps per second: 63, episode reward: -168.939, mean reward: -0.338 [-3.779, -0.014], mean action: 0.270 [-0.846, 1.319], mean observation: 0.081 [-427.884, 1000.000], loss: 0.547866, mean_absolute_error: 0.723251, mean_q: -19.696505\n",
      " 65000/100000: episode: 130, duration: 7.623s, episode steps: 500, steps per second: 66, episode reward: -166.650, mean reward: -0.333 [-3.776, -0.013], mean action: 0.435 [-0.425, 1.759], mean observation: 0.014 [-1000.000, 1000.000], loss: 0.536658, mean_absolute_error: 0.711301, mean_q: -19.143528\n",
      " 65500/100000: episode: 131, duration: 8.783s, episode steps: 500, steps per second: 57, episode reward: -191.230, mean reward: -0.382 [-3.778, -0.002], mean action: 0.494 [-0.484, 1.229], mean observation: -0.011 [-15.575, 11.175], loss: 0.490349, mean_absolute_error: 0.660041, mean_q: -19.764219\n",
      " 66000/100000: episode: 132, duration: 9.201s, episode steps: 500, steps per second: 54, episode reward: -228.386, mean reward: -0.457 [-3.778, -0.000], mean action: 0.405 [-0.559, 1.365], mean observation: -0.013 [-17.642, 9.911], loss: 0.508463, mean_absolute_error: 0.681674, mean_q: -19.522352\n",
      " 66500/100000: episode: 133, duration: 7.761s, episode steps: 500, steps per second: 64, episode reward: -212.598, mean reward: -0.425 [-3.778, -0.007], mean action: 0.528 [-0.258, 1.283], mean observation: 0.332 [-517.177, 1000.000], loss: 0.565077, mean_absolute_error: 0.737146, mean_q: -19.106367\n",
      " 67000/100000: episode: 134, duration: 11.065s, episode steps: 500, steps per second: 45, episode reward: -170.361, mean reward: -0.341 [-3.777, -0.011], mean action: 0.345 [-0.599, 1.515], mean observation: 0.027 [-17.712, 9.348], loss: 0.566827, mean_absolute_error: 0.742674, mean_q: -19.444660\n",
      " 67500/100000: episode: 135, duration: 8.908s, episode steps: 500, steps per second: 56, episode reward: -184.164, mean reward: -0.368 [-3.775, -0.006], mean action: 0.441 [-0.310, 1.271], mean observation: -0.031 [-1000.000, 1000.000], loss: 0.514926, mean_absolute_error: 0.691658, mean_q: -19.364996\n",
      " 68000/100000: episode: 136, duration: 13.639s, episode steps: 500, steps per second: 37, episode reward: -168.773, mean reward: -0.338 [-3.777, -0.007], mean action: 0.355 [-0.338, 1.529], mean observation: 0.021 [-1000.000, 1000.000], loss: 0.518962, mean_absolute_error: 0.694048, mean_q: -19.562582\n",
      " 68500/100000: episode: 137, duration: 11.680s, episode steps: 500, steps per second: 43, episode reward: -185.681, mean reward: -0.371 [-3.780, -0.005], mean action: 0.270 [-0.575, 1.401], mean observation: 0.021 [-343.728, 345.778], loss: 0.514485, mean_absolute_error: 0.683910, mean_q: -18.869564\n",
      " 69000/100000: episode: 138, duration: 7.833s, episode steps: 500, steps per second: 64, episode reward: -172.394, mean reward: -0.345 [-3.779, -0.010], mean action: 0.316 [-0.395, 1.297], mean observation: 0.011 [-17.624, 11.483], loss: 0.503970, mean_absolute_error: 0.675384, mean_q: -18.717739\n",
      " 69500/100000: episode: 139, duration: 13.226s, episode steps: 500, steps per second: 38, episode reward: -201.951, mean reward: -0.404 [-3.779, -0.019], mean action: 0.366 [-0.362, 1.853], mean observation: 0.033 [-15.294, 10.207], loss: 0.541780, mean_absolute_error: 0.713658, mean_q: -18.730831\n",
      " 70000/100000: episode: 140, duration: 7.858s, episode steps: 500, steps per second: 64, episode reward: -188.321, mean reward: -0.377 [-3.777, -0.028], mean action: 0.444 [-0.441, 1.514], mean observation: 0.005 [-17.755, 11.170], loss: 0.541744, mean_absolute_error: 0.708412, mean_q: -19.238197\n",
      " 70500/100000: episode: 141, duration: 9.379s, episode steps: 500, steps per second: 53, episode reward: -160.967, mean reward: -0.322 [-3.777, -0.001], mean action: 0.254 [-0.785, 1.587], mean observation: 0.015 [-1000.000, 1000.000], loss: 0.550550, mean_absolute_error: 0.723903, mean_q: -18.741478\n",
      " 71000/100000: episode: 142, duration: 8.480s, episode steps: 500, steps per second: 59, episode reward: -158.845, mean reward: -0.318 [-3.779, -0.035], mean action: 0.331 [-0.539, 1.495], mean observation: 0.036 [-1000.000, 1000.000], loss: 0.497016, mean_absolute_error: 0.668175, mean_q: -19.018417\n",
      " 71500/100000: episode: 143, duration: 8.645s, episode steps: 500, steps per second: 58, episode reward: -176.770, mean reward: -0.354 [-3.779, -0.022], mean action: 0.184 [-0.913, 1.274], mean observation: -0.027 [-1000.000, 804.245], loss: 0.540025, mean_absolute_error: 0.706295, mean_q: -19.027735\n",
      " 72000/100000: episode: 144, duration: 8.221s, episode steps: 500, steps per second: 61, episode reward: -109.035, mean reward: -0.218 [-3.778, -0.020], mean action: 0.148 [-0.441, 1.308], mean observation: 0.020 [-1000.000, 887.815], loss: 0.486480, mean_absolute_error: 0.657132, mean_q: -18.547628\n",
      " 72500/100000: episode: 145, duration: 10.096s, episode steps: 500, steps per second: 50, episode reward: -118.835, mean reward: -0.238 [-3.776, -0.009], mean action: 0.097 [-0.518, 1.245], mean observation: -0.004 [-1000.000, 734.411], loss: 0.509581, mean_absolute_error: 0.677099, mean_q: -19.166042\n",
      " 73000/100000: episode: 146, duration: 9.525s, episode steps: 500, steps per second: 52, episode reward: -165.574, mean reward: -0.331 [-3.777, -0.006], mean action: 0.392 [-0.293, 1.712], mean observation: 0.032 [-17.661, 13.346], loss: 0.580932, mean_absolute_error: 0.749144, mean_q: -19.250959\n",
      " 73500/100000: episode: 147, duration: 10.372s, episode steps: 500, steps per second: 48, episode reward: -179.033, mean reward: -0.358 [-3.777, -0.002], mean action: 0.402 [-0.686, 1.730], mean observation: 0.018 [-1000.000, 1000.000], loss: 0.519465, mean_absolute_error: 0.695381, mean_q: -18.887774\n",
      " 74000/100000: episode: 148, duration: 8.371s, episode steps: 500, steps per second: 60, episode reward: -151.623, mean reward: -0.303 [-3.778, -0.000], mean action: 0.042 [-0.626, 1.415], mean observation: 0.116 [-17.632, 268.944], loss: 0.515737, mean_absolute_error: 0.687954, mean_q: -19.072903\n",
      " 74500/100000: episode: 149, duration: 10.757s, episode steps: 500, steps per second: 46, episode reward: -168.234, mean reward: -0.336 [-3.776, -0.010], mean action: 0.219 [-0.595, 1.537], mean observation: 0.043 [-18.098, 10.908], loss: 0.528372, mean_absolute_error: 0.699156, mean_q: -18.913082\n",
      " 75000/100000: episode: 150, duration: 10.050s, episode steps: 500, steps per second: 50, episode reward: -151.315, mean reward: -0.303 [-3.776, -0.020], mean action: 0.185 [-0.432, 1.380], mean observation: -0.055 [-1000.000, 639.424], loss: 0.530382, mean_absolute_error: 0.702005, mean_q: -19.249781\n",
      " 75500/100000: episode: 151, duration: 9.820s, episode steps: 500, steps per second: 51, episode reward: -157.084, mean reward: -0.314 [-3.777, -0.000], mean action: 0.328 [-0.598, 1.246], mean observation: 0.217 [-1000.000, 1000.000], loss: 0.476442, mean_absolute_error: 0.643631, mean_q: -18.505405\n",
      " 76000/100000: episode: 152, duration: 9.943s, episode steps: 500, steps per second: 50, episode reward: -150.975, mean reward: -0.302 [-3.778, -0.001], mean action: 0.268 [-0.722, 1.545], mean observation: -0.010 [-1000.000, 826.045], loss: 0.481438, mean_absolute_error: 0.651399, mean_q: -18.516859\n",
      " 76500/100000: episode: 153, duration: 8.807s, episode steps: 500, steps per second: 57, episode reward: -212.724, mean reward: -0.425 [-3.781, -0.041], mean action: 0.497 [-0.303, 2.080], mean observation: -0.015 [-18.628, 11.358], loss: 0.476305, mean_absolute_error: 0.643474, mean_q: -19.044359\n",
      " 77000/100000: episode: 154, duration: 8.715s, episode steps: 500, steps per second: 57, episode reward: -205.670, mean reward: -0.411 [-3.778, -0.006], mean action: 0.509 [-0.511, 1.460], mean observation: -0.022 [-901.055, 569.322], loss: 0.478351, mean_absolute_error: 0.645445, mean_q: -18.657227\n",
      " 77500/100000: episode: 155, duration: 7.581s, episode steps: 500, steps per second: 66, episode reward: -163.585, mean reward: -0.327 [-3.779, -0.029], mean action: 0.504 [-0.355, 1.620], mean observation: -0.023 [-1000.000, 779.618], loss: 0.481487, mean_absolute_error: 0.653165, mean_q: -18.683681\n",
      " 78000/100000: episode: 156, duration: 8.061s, episode steps: 500, steps per second: 62, episode reward: -141.266, mean reward: -0.283 [-3.775, -0.000], mean action: 0.238 [-0.520, 1.482], mean observation: -0.093 [-1000.000, 635.720], loss: 0.456054, mean_absolute_error: 0.621185, mean_q: -18.748722\n",
      " 78500/100000: episode: 157, duration: 8.797s, episode steps: 500, steps per second: 57, episode reward: -201.832, mean reward: -0.404 [-3.777, -0.001], mean action: 0.225 [-0.532, 1.130], mean observation: -0.005 [-1000.000, 1000.000], loss: 0.546576, mean_absolute_error: 0.712080, mean_q: -18.906927\n",
      " 79000/100000: episode: 158, duration: 10.836s, episode steps: 500, steps per second: 46, episode reward: -234.317, mean reward: -0.469 [-3.776, -0.001], mean action: 0.485 [-0.284, 1.447], mean observation: -0.019 [-20.086, 12.277], loss: 0.484518, mean_absolute_error: 0.654658, mean_q: -18.672066\n",
      " 79500/100000: episode: 159, duration: 11.554s, episode steps: 500, steps per second: 43, episode reward: -128.643, mean reward: -0.257 [-3.777, -0.001], mean action: 0.213 [-0.404, 1.221], mean observation: -0.024 [-1000.000, 577.040], loss: 0.455371, mean_absolute_error: 0.619884, mean_q: -18.874899\n",
      " 80000/100000: episode: 160, duration: 9.510s, episode steps: 500, steps per second: 53, episode reward: -183.101, mean reward: -0.366 [-3.777, -0.011], mean action: 0.408 [-0.376, 1.498], mean observation: 0.063 [-647.927, 1000.000], loss: 0.477704, mean_absolute_error: 0.646604, mean_q: -18.568007\n",
      " 80500/100000: episode: 161, duration: 8.587s, episode steps: 500, steps per second: 58, episode reward: -172.196, mean reward: -0.344 [-3.775, -0.005], mean action: 0.345 [-0.585, 1.500], mean observation: 0.025 [-1000.000, 1000.000], loss: 0.496617, mean_absolute_error: 0.665839, mean_q: -18.667425\n",
      " 81000/100000: episode: 162, duration: 11.151s, episode steps: 500, steps per second: 45, episode reward: -119.104, mean reward: -0.238 [-3.776, -0.008], mean action: 0.202 [-0.640, 1.438], mean observation: 0.044 [-1000.000, 1000.000], loss: 0.483235, mean_absolute_error: 0.651541, mean_q: -18.667097\n",
      " 81500/100000: episode: 163, duration: 8.775s, episode steps: 500, steps per second: 57, episode reward: -123.023, mean reward: -0.246 [-3.776, -0.009], mean action: 0.415 [-0.503, 1.383], mean observation: 0.037 [-393.472, 362.505], loss: 0.451319, mean_absolute_error: 0.614901, mean_q: -18.848963\n",
      " 82000/100000: episode: 164, duration: 10.796s, episode steps: 500, steps per second: 46, episode reward: -102.893, mean reward: -0.206 [-3.776, -0.000], mean action: 0.251 [-0.640, 1.470], mean observation: -0.014 [-1000.000, 846.469], loss: 0.508972, mean_absolute_error: 0.677729, mean_q: -18.754961\n",
      " 82500/100000: episode: 165, duration: 10.270s, episode steps: 500, steps per second: 49, episode reward: -178.709, mean reward: -0.357 [-3.777, -0.010], mean action: 0.404 [-0.375, 1.513], mean observation: 0.031 [-1000.000, 931.616], loss: 0.417766, mean_absolute_error: 0.582838, mean_q: -18.848766\n",
      " 83000/100000: episode: 166, duration: 11.928s, episode steps: 500, steps per second: 42, episode reward: -112.735, mean reward: -0.225 [-3.778, -0.009], mean action: 0.242 [-0.478, 1.307], mean observation: 0.303 [-1000.000, 1000.000], loss: 0.403071, mean_absolute_error: 0.568330, mean_q: -18.234566\n",
      " 83500/100000: episode: 167, duration: 10.471s, episode steps: 500, steps per second: 48, episode reward: -88.709, mean reward: -0.177 [-3.779, -0.001], mean action: 0.418 [-0.369, 1.364], mean observation: 0.067 [-17.683, 32.294], loss: 0.448695, mean_absolute_error: 0.613694, mean_q: -18.776424\n",
      " 84000/100000: episode: 168, duration: 8.807s, episode steps: 500, steps per second: 57, episode reward: -215.476, mean reward: -0.431 [-3.777, -0.016], mean action: 0.509 [-0.453, 1.468], mean observation: -0.016 [-17.159, 11.485], loss: 0.418099, mean_absolute_error: 0.579017, mean_q: -18.344425\n",
      " 84500/100000: episode: 169, duration: 8.765s, episode steps: 500, steps per second: 57, episode reward: -161.129, mean reward: -0.322 [-3.779, -0.014], mean action: 0.384 [-0.801, 1.663], mean observation: -0.009 [-1000.000, 865.152], loss: 0.474249, mean_absolute_error: 0.639515, mean_q: -18.918406\n",
      " 85000/100000: episode: 170, duration: 9.286s, episode steps: 500, steps per second: 54, episode reward: -208.644, mean reward: -0.417 [-3.776, -0.006], mean action: 0.526 [-0.239, 1.471], mean observation: -0.002 [-341.329, 345.652], loss: 0.410777, mean_absolute_error: 0.573394, mean_q: -18.172407\n",
      " 85500/100000: episode: 171, duration: 11.600s, episode steps: 500, steps per second: 43, episode reward: -119.326, mean reward: -0.239 [-3.777, -0.005], mean action: 0.340 [-0.579, 1.282], mean observation: 0.064 [-1000.000, 1000.000], loss: 0.426173, mean_absolute_error: 0.594308, mean_q: -18.678652\n",
      " 86000/100000: episode: 172, duration: 11.702s, episode steps: 500, steps per second: 43, episode reward: -140.304, mean reward: -0.281 [-3.778, -0.001], mean action: 0.195 [-0.832, 1.593], mean observation: 0.062 [-17.673, 8.998], loss: 0.459496, mean_absolute_error: 0.624249, mean_q: -18.573414\n",
      " 86500/100000: episode: 173, duration: 9.148s, episode steps: 500, steps per second: 55, episode reward: -151.742, mean reward: -0.303 [-3.778, -0.012], mean action: 0.248 [-0.586, 1.466], mean observation: 0.137 [-257.656, 1000.000], loss: 0.481308, mean_absolute_error: 0.645121, mean_q: -18.378805\n",
      " 87000/100000: episode: 174, duration: 9.089s, episode steps: 500, steps per second: 55, episode reward: -115.235, mean reward: -0.230 [-3.778, -0.008], mean action: 0.281 [-0.749, 1.399], mean observation: 0.062 [-17.701, 8.514], loss: 0.447389, mean_absolute_error: 0.611079, mean_q: -18.638474\n",
      " 87500/100000: episode: 175, duration: 11.660s, episode steps: 500, steps per second: 43, episode reward: -128.418, mean reward: -0.257 [-3.776, -0.010], mean action: 0.383 [-0.297, 1.381], mean observation: -0.065 [-1000.000, 1000.000], loss: 0.447552, mean_absolute_error: 0.615815, mean_q: -18.492504\n",
      " 88000/100000: episode: 176, duration: 8.977s, episode steps: 500, steps per second: 56, episode reward: -182.846, mean reward: -0.366 [-3.775, -0.045], mean action: 0.254 [-0.704, 1.274], mean observation: 0.159 [-1000.000, 1000.000], loss: 0.469780, mean_absolute_error: 0.632949, mean_q: -18.725962\n",
      " 88500/100000: episode: 177, duration: 9.150s, episode steps: 500, steps per second: 55, episode reward: -104.278, mean reward: -0.209 [-3.779, -0.019], mean action: 0.219 [-0.706, 1.631], mean observation: 0.069 [-17.543, 13.537], loss: 0.414086, mean_absolute_error: 0.578372, mean_q: -18.456993\n",
      " 89000/100000: episode: 178, duration: 8.422s, episode steps: 500, steps per second: 59, episode reward: -99.542, mean reward: -0.199 [-3.775, -0.012], mean action: 0.379 [-0.416, 1.395], mean observation: 0.062 [-17.919, 10.039], loss: 0.405746, mean_absolute_error: 0.570336, mean_q: -18.104876\n",
      " 89500/100000: episode: 179, duration: 9.285s, episode steps: 500, steps per second: 54, episode reward: -147.726, mean reward: -0.295 [-3.779, -0.001], mean action: 0.322 [-0.422, 1.204], mean observation: 0.027 [-1000.000, 1000.000], loss: 0.504283, mean_absolute_error: 0.671311, mean_q: -18.432381\n",
      " 90000/100000: episode: 180, duration: 7.486s, episode steps: 500, steps per second: 67, episode reward: -234.036, mean reward: -0.468 [-3.776, -0.013], mean action: 0.373 [-0.570, 1.394], mean observation: 0.076 [-611.436, 1000.000], loss: 0.416954, mean_absolute_error: 0.582551, mean_q: -18.545916\n",
      " 90500/100000: episode: 181, duration: 10.369s, episode steps: 500, steps per second: 48, episode reward: -122.231, mean reward: -0.244 [-3.776, -0.002], mean action: 0.340 [-0.589, 1.472], mean observation: 0.054 [-16.100, 9.839], loss: 0.405142, mean_absolute_error: 0.571028, mean_q: -18.284554\n",
      " 91000/100000: episode: 182, duration: 8.351s, episode steps: 500, steps per second: 60, episode reward: -121.486, mean reward: -0.243 [-3.777, -0.008], mean action: 0.293 [-0.785, 1.613], mean observation: 0.034 [-17.641, 8.480], loss: 0.418716, mean_absolute_error: 0.580277, mean_q: -18.405207\n",
      " 91500/100000: episode: 183, duration: 8.229s, episode steps: 500, steps per second: 61, episode reward: -189.882, mean reward: -0.380 [-3.778, -0.006], mean action: 0.181 [-0.839, 1.281], mean observation: -0.007 [-1000.000, 1000.000], loss: 0.405025, mean_absolute_error: 0.569594, mean_q: -18.328453\n",
      " 92000/100000: episode: 184, duration: 8.411s, episode steps: 500, steps per second: 59, episode reward: -114.973, mean reward: -0.230 [-3.778, -0.003], mean action: 0.371 [-0.414, 1.646], mean observation: 0.062 [-17.715, 11.341], loss: 0.385488, mean_absolute_error: 0.551348, mean_q: -18.750475\n",
      " 92500/100000: episode: 185, duration: 8.268s, episode steps: 500, steps per second: 60, episode reward: -172.883, mean reward: -0.346 [-3.773, -0.008], mean action: 0.385 [-1.010, 1.743], mean observation: 0.007 [-18.089, 11.086], loss: 0.383491, mean_absolute_error: 0.543216, mean_q: -18.226988\n",
      " 93000/100000: episode: 186, duration: 10.006s, episode steps: 500, steps per second: 50, episode reward: -152.862, mean reward: -0.306 [-3.777, -0.000], mean action: 0.294 [-0.659, 1.340], mean observation: -0.032 [-1000.000, 731.661], loss: 0.442176, mean_absolute_error: 0.599736, mean_q: -18.280684\n",
      " 93500/100000: episode: 187, duration: 8.804s, episode steps: 500, steps per second: 57, episode reward: -142.745, mean reward: -0.285 [-3.779, -0.000], mean action: 0.265 [-0.530, 1.372], mean observation: 0.018 [-1000.000, 1000.000], loss: 0.399162, mean_absolute_error: 0.561649, mean_q: -18.097515\n",
      " 94000/100000: episode: 188, duration: 8.882s, episode steps: 500, steps per second: 56, episode reward: -141.304, mean reward: -0.283 [-3.779, -0.006], mean action: 0.381 [-0.547, 1.191], mean observation: 0.037 [-17.486, 12.387], loss: 0.386741, mean_absolute_error: 0.547440, mean_q: -18.113550\n",
      " 94500/100000: episode: 189, duration: 8.748s, episode steps: 500, steps per second: 57, episode reward: -74.826, mean reward: -0.150 [-3.780, -0.001], mean action: 0.247 [-0.630, 1.266], mean observation: 0.051 [-1000.000, 828.872], loss: 0.405924, mean_absolute_error: 0.566877, mean_q: -18.595181\n",
      " 95000/100000: episode: 190, duration: 9.534s, episode steps: 500, steps per second: 52, episode reward: -99.903, mean reward: -0.200 [-3.778, -0.000], mean action: 0.272 [-0.685, 1.467], mean observation: 0.041 [-16.866, 12.707], loss: 0.437968, mean_absolute_error: 0.599504, mean_q: -18.610027\n",
      " 95500/100000: episode: 191, duration: 7.391s, episode steps: 500, steps per second: 68, episode reward: -172.885, mean reward: -0.346 [-3.776, -0.002], mean action: 0.199 [-0.595, 1.341], mean observation: -0.006 [-15.504, 11.151], loss: 0.444801, mean_absolute_error: 0.600608, mean_q: -18.284594\n",
      " 96000/100000: episode: 192, duration: 11.706s, episode steps: 500, steps per second: 43, episode reward: -126.614, mean reward: -0.253 [-3.777, -0.000], mean action: 0.206 [-0.466, 1.353], mean observation: 0.040 [-477.482, 437.354], loss: 0.421216, mean_absolute_error: 0.579655, mean_q: -18.053547\n",
      " 96500/100000: episode: 193, duration: 12.803s, episode steps: 500, steps per second: 39, episode reward: -108.680, mean reward: -0.217 [-3.781, -0.001], mean action: 0.098 [-1.153, 1.399], mean observation: 0.075 [-16.575, 10.468], loss: 0.418610, mean_absolute_error: 0.577252, mean_q: -18.477755\n",
      " 97000/100000: episode: 194, duration: 9.789s, episode steps: 500, steps per second: 51, episode reward: -123.969, mean reward: -0.248 [-3.774, -0.002], mean action: 0.396 [-0.491, 1.462], mean observation: 0.033 [-16.789, 12.226], loss: 0.485172, mean_absolute_error: 0.647212, mean_q: -18.438559\n",
      " 97500/100000: episode: 195, duration: 9.553s, episode steps: 500, steps per second: 52, episode reward: -124.872, mean reward: -0.250 [-3.777, -0.000], mean action: 0.295 [-0.406, 1.420], mean observation: 0.012 [-1000.000, 690.055], loss: 0.417210, mean_absolute_error: 0.577705, mean_q: -18.214911\n",
      " 98000/100000: episode: 196, duration: 11.321s, episode steps: 500, steps per second: 44, episode reward: -111.029, mean reward: -0.222 [-3.777, -0.002], mean action: 0.377 [-0.531, 1.632], mean observation: 0.221 [-16.730, 1000.000], loss: 0.379031, mean_absolute_error: 0.538217, mean_q: -18.094130\n",
      " 98500/100000: episode: 197, duration: 9.265s, episode steps: 500, steps per second: 54, episode reward: -115.186, mean reward: -0.230 [-3.774, -0.000], mean action: 0.404 [-0.704, 1.900], mean observation: 0.181 [-17.852, 1000.000], loss: 0.408676, mean_absolute_error: 0.566222, mean_q: -18.229624\n",
      " 99000/100000: episode: 198, duration: 8.983s, episode steps: 500, steps per second: 56, episode reward: -97.791, mean reward: -0.196 [-3.776, -0.000], mean action: 0.287 [-0.677, 1.400], mean observation: 0.134 [-1000.000, 1000.000], loss: 0.382964, mean_absolute_error: 0.538127, mean_q: -18.179230\n",
      " 99500/100000: episode: 199, duration: 12.816s, episode steps: 500, steps per second: 39, episode reward: -200.229, mean reward: -0.400 [-3.776, -0.002], mean action: 0.583 [-0.255, 1.841], mean observation: -0.368 [-1000.000, 1000.000], loss: 0.369237, mean_absolute_error: 0.525836, mean_q: -18.271509\n",
      " 100000/100000: episode: 200, duration: 12.213s, episode steps: 500, steps per second: 41, episode reward: -167.083, mean reward: -0.334 [-3.774, -0.019], mean action: 0.442 [-0.391, 1.748], mean observation: 1.137 [-1000.000, 1000.000], loss: 0.472719, mean_absolute_error: 0.625324, mean_q: -18.008665\n",
      "done, took 2202.661 seconds\n"
     ]
    }
   ],
   "source": [
    "       \n",
    "        \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for i in range(5000):\n",
    "    observation, reward, done, info = env.step([0,0,0,0,0,0])#env.action_space.sample())\n",
    "    #observation, reward, done, info = env.step(0.55*np.random.randint(0,2,6))#env.action_space.sample())\n",
    "    #print(observation[2:4])\n",
    "    print(observation[2])\n",
    "\"\"\"\n",
    "    \n",
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Total number of steps in training\n",
    "nallsteps = args.steps\n",
    "\n",
    "# Next, we build a very simple model.\n",
    "# Create networks for DDPG\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('sigmoid'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = merge([action_input, flattened_observation], mode='concat')\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(input=[action_input, observation_input], output=x)\n",
    "print(critic.summary())\n",
    "\n",
    "history = LossHistory()\n",
    "\n",
    "# Set up the agent for training\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.2, size=env.noutput)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  delta_clip=1.)\n",
    "# agent = ContinuousDQNAgent(nb_actions=env.noutput, V_model=V_model, L_model=L_model, mu_model=mu_model,\n",
    "#                            memory=memory, nb_steps_warmup=1000, random_process=random_process,\n",
    "#                            gamma=.99, target_model_update=0.1)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n",
    "if args.train:\n",
    "    agent.fit(env, nb_steps=nallsteps, visualize=False, verbose=2, nb_max_episode_steps=env.timestep_limit, log_interval=10000)\n",
    "    \n",
    "    #print history.losses\n",
    "    # After training is done, we save the final weights.\n",
    "    agent.save_weights(args.model, overwrite=True)\n",
    "\n",
    "if not args.train:\n",
    "    agent.load_weights(args.model)\n",
    "    # Finally, evaluate our algorithm for 1 episode.\n",
    "    agent.test(env, nb_episodes=1, visualize=False, nb_max_episode_steps=500, callbacks=[history])\n",
    "    lst2 = [item[2] for item in history.losses]\n",
    "    lst3 = [item[3] for item in history.losses]\n",
    "    lst4 = [item[4] for item in history.losses]\n",
    "    lst5 = [item[5] for item in history.losses]\n",
    "    #print(lst2)\n",
    "    plt.plot(lst2)\n",
    "    #plt.plot(lst3)\n",
    "    plt.plot(lst4)\n",
    "    #plt.plot(lst5)\n",
    "    #plt.plot(history.losses[0])\n",
    "    plt.show()\n",
    "    #print history.losses    \n",
    "    #print(x)\n",
    "    #for i in range(5000):\n",
    "    #    observation, reward, done, info = env.step(env.action_space)#env.action_space.sample())\n",
    "    #    print(observation)\n",
    "    #agent.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import opensim as osim\n",
    "\n",
    "print(osim.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opensim-rl)",
   "language": "python",
   "name": "opensim-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
