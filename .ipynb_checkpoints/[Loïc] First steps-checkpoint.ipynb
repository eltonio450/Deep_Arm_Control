{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of importation\n"
     ]
    }
   ],
   "source": [
    "# Derived from keras-rl\n",
    "import opensim as osim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, merge\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "from CustomArmEnv2 import CustomArmEnv2\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "print(\"End of importation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments...\n",
      "Namespace(model='example.h5f', steps=10000, visualize=False)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "std::exception in 'OpenSim::Model::Model(std::string const &)': Object: Cannot not open file ../models/arm2dof6musc.osim. It may not exist or you do not have permission to read it.\n\tIn file /home/lukasz/anaconda2/conda-bld/opensim_1485565446036/work/OpenSim/Common/Object.cpp:104\n\tIn function 'Object'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1678688dca69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Load walking environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomArmEnv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Environment generated\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/antoine/Notebooks/Deep_Arm_Control/CustomArmEnv2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, visualize)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# task is divided in two period, reaching the target (counter = 0) and comming back to initial position (counter = 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomArmEnv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/antoine/anaconda3/envs/opensim-rl/lib/python2.7/site-packages/osim/env/osim.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, visualize, noutput)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mosim_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOsim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/antoine/anaconda3/envs/opensim-rl/lib/python2.7/site-packages/osim/env/osim.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_path, visualize)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Enable the visualizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/antoine/anaconda3/envs/opensim-rl/lib/python2.7/site-packages/opensim/simulation.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m  54258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  54259\u001b[0m         \"\"\"\n\u001b[0;32m> 54260\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_simulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_Model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  54261\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  54262\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: std::exception in 'OpenSim::Model::Model(std::string const &)': Object: Cannot not open file ../models/arm2dof6musc.osim. It may not exist or you do not have permission to read it.\n\tIn file /home/lukasz/anaconda2/conda-bld/opensim_1485565446036/work/OpenSim/Common/Object.cpp:104\n\tIn function 'Object'"
     ]
    }
   ],
   "source": [
    "print(\"Arguments...\")\n",
    "#print(args)\n",
    "\n",
    "args = argparse.Namespace(model='example.h5f', steps=10000, visualize=False)\n",
    "print(args)\n",
    "# Load walking environment\n",
    "env = CustomArmEnv2(args.visualize)\n",
    "print(\"Environment generated\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_1 (Flatten)              (None, 14)            0           flatten_input_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 32)            480         flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 32)            0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 32)            1056        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 32)            0           dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 32)            1056        activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 32)            0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 6)             198         activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 6)             0           dense_4[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 2790\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 14)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 14)            0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, 20)            0           action_input[0][0]               \n",
      "                                                                   flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 64)            1344        merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 64)            0           dense_5[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_6 (Dense)                  (None, 64)            4160        activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 64)            0           dense_6[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_7 (Dense)                  (None, 64)            4160        activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, 64)            0           dense_7[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_8 (Dense)                  (None, 1)             65          activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, 1)             0           dense_8[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 9729\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Total number of steps in training\n",
    "nallsteps = args.steps\n",
    "\n",
    "# Create networks for DDPG\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(32, activation='relu'))\n",
    "actor.add(Dense(32, activation='relu'))\n",
    "actor.add(Dense(32, activation='relu'))\n",
    "actor.add(Dense(nb_actions, activation='sigmoid'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = merge([action_input, flattened_observation], mode='concat')\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(1, activation='linear')(x)\n",
    "critic = Model(input=[action_input, observation_input], output=x)\n",
    "print(critic.summary())\n",
    "\n",
    "# Set up the agent for training\n",
    "#warning: a too high limit gives errors\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.2, size=env.noutput)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  delta_clip=1.)\n",
    "# agent = ContinuousDQNAgent(nb_actions=env.noutput, V_model=V_model, L_model=L_model, mu_model=mu_model,\n",
    "#                            memory=memory, nb_steps_warmup=1000, random_process=random_process,\n",
    "#                            gamma=.99, target_model_update=0.1)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  500/10000: episode: 1, duration: 7.545s, episode steps: 500, steps per second: 66, episode reward: -514.495, mean reward: -1.029 [-3.954, -0.032], mean action: 0.415 [-0.405, 1.259], mean observation: -0.180 [-1000.000, 521.844], loss: 2.034031, mean_absolute_error: 2.177257, mean_q: -1.533683\n",
      " 1000/10000: episode: 2, duration: 8.129s, episode steps: 500, steps per second: 62, episode reward: -1532.841, mean reward: -3.066 [-3.799, -0.065], mean action: 0.127 [-0.656, 1.175], mean observation: -0.170 [-1000.000, 1000.000], loss: 1.974462, mean_absolute_error: 2.170917, mean_q: -3.879257\n",
      " 1500/10000: episode: 3, duration: 14.944s, episode steps: 500, steps per second: 33, episode reward: -1208.881, mean reward: -2.418 [-3.600, -0.534], mean action: 0.366 [-0.330, 1.520], mean observation: 0.077 [-562.709, 663.677], loss: 0.850026, mean_absolute_error: 1.021044, mean_q: -4.471915\n",
      " 2000/10000: episode: 4, duration: 14.296s, episode steps: 500, steps per second: 35, episode reward: -921.091, mean reward: -1.842 [-4.445, -0.015], mean action: 0.056 [-0.736, 1.192], mean observation: -0.684 [-400.922, 1000.000], loss: 0.518261, mean_absolute_error: 0.702966, mean_q: -5.424554\n",
      " 2500/10000: episode: 5, duration: 18.698s, episode steps: 500, steps per second: 27, episode reward: -823.009, mean reward: -1.646 [-3.131, -0.254], mean action: 0.169 [-0.374, 1.326], mean observation: -0.208 [-250.992, 275.553], loss: 0.372203, mean_absolute_error: 0.551088, mean_q: -6.509960\n",
      " 3000/10000: episode: 6, duration: 17.041s, episode steps: 500, steps per second: 29, episode reward: -828.962, mean reward: -1.658 [-2.863, -0.520], mean action: 0.239 [-0.255, 1.307], mean observation: -0.134 [-375.941, 819.899], loss: 0.290570, mean_absolute_error: 0.466030, mean_q: -6.662648\n",
      " 3500/10000: episode: 7, duration: 20.183s, episode steps: 500, steps per second: 25, episode reward: -659.593, mean reward: -1.319 [-3.704, -0.329], mean action: 0.004 [-0.878, 1.410], mean observation: -0.432 [-20.210, 457.146], loss: 0.224375, mean_absolute_error: 0.397319, mean_q: -7.183912\n",
      " 4000/10000: episode: 8, duration: 21.879s, episode steps: 500, steps per second: 23, episode reward: -1265.405, mean reward: -2.531 [-3.828, -0.054], mean action: 0.098 [-0.687, 1.075], mean observation: -0.549 [-1000.000, 1000.000], loss: 0.389703, mean_absolute_error: 0.575149, mean_q: -8.178092\n",
      " 4500/10000: episode: 9, duration: 11.837s, episode steps: 500, steps per second: 42, episode reward: -722.606, mean reward: -1.445 [-3.669, -0.293], mean action: 0.152 [-0.457, 1.154], mean observation: -0.534 [-17.923, 8.408], loss: 0.368169, mean_absolute_error: 0.562430, mean_q: -8.959095\n",
      " 5000/10000: episode: 10, duration: 24.933s, episode steps: 500, steps per second: 20, episode reward: -803.560, mean reward: -1.607 [-2.214, -0.314], mean action: 0.309 [-0.434, 1.249], mean observation: -0.173 [-10.730, 133.677], loss: 0.313728, mean_absolute_error: 0.504497, mean_q: -9.602213\n",
      " 5500/10000: episode: 11, duration: 13.198s, episode steps: 500, steps per second: 38, episode reward: -1216.601, mean reward: -2.433 [-3.721, -0.135], mean action: 0.115 [-0.479, 1.083], mean observation: -0.433 [-21.901, 154.121], loss: 0.260134, mean_absolute_error: 0.457749, mean_q: -10.243440\n",
      " 6000/10000: episode: 12, duration: 11.233s, episode steps: 500, steps per second: 45, episode reward: -469.690, mean reward: -0.939 [-1.599, -0.149], mean action: 0.676 [-0.324, 1.453], mean observation: -0.034 [-13.778, 220.124], loss: 0.224361, mean_absolute_error: 0.412380, mean_q: -10.941166\n",
      " 6500/10000: episode: 13, duration: 9.128s, episode steps: 500, steps per second: 55, episode reward: -1020.381, mean reward: -2.041 [-3.564, -0.614], mean action: 0.632 [-0.278, 1.940], mean observation: -0.208 [-121.835, 810.748], loss: 0.239509, mean_absolute_error: 0.433701, mean_q: -11.122118\n",
      " 7000/10000: episode: 14, duration: 12.451s, episode steps: 500, steps per second: 40, episode reward: -866.795, mean reward: -1.734 [-4.404, -0.752], mean action: 0.220 [-0.523, 1.240], mean observation: 0.293 [-13.195, 11.340], loss: 0.239139, mean_absolute_error: 0.437223, mean_q: -11.921739\n",
      " 7500/10000: episode: 15, duration: 11.461s, episode steps: 500, steps per second: 44, episode reward: -364.606, mean reward: -0.729 [-1.877, -0.262], mean action: 0.475 [-0.442, 1.340], mean observation: 2.245 [-1000.000, 1000.000], loss: 0.245855, mean_absolute_error: 0.444508, mean_q: -12.520028\n",
      " 8000/10000: episode: 16, duration: 18.075s, episode steps: 500, steps per second: 28, episode reward: -1176.858, mean reward: -2.354 [-3.590, -0.206], mean action: 0.378 [-0.547, 1.494], mean observation: -0.073 [-1000.000, 1000.000], loss: 0.340140, mean_absolute_error: 0.545789, mean_q: -12.790359\n",
      " 8500/10000: episode: 17, duration: 10.185s, episode steps: 500, steps per second: 49, episode reward: -834.332, mean reward: -1.669 [-2.086, -0.444], mean action: 0.525 [-0.929, 1.565], mean observation: -0.185 [-12.177, 106.541], loss: 0.368035, mean_absolute_error: 0.586576, mean_q: -13.284888\n",
      " 9000/10000: episode: 18, duration: 11.955s, episode steps: 500, steps per second: 42, episode reward: -687.978, mean reward: -1.376 [-3.321, -0.542], mean action: 0.319 [-0.415, 1.380], mean observation: -0.533 [-603.821, 303.934], loss: 0.368650, mean_absolute_error: 0.589502, mean_q: -13.905007\n",
      " 9500/10000: episode: 19, duration: 10.388s, episode steps: 500, steps per second: 48, episode reward: -725.475, mean reward: -1.451 [-2.117, -0.016], mean action: 0.495 [-0.454, 1.630], mean observation: -0.220 [-14.628, 78.289], loss: 0.339993, mean_absolute_error: 0.564512, mean_q: -14.499794\n",
      " 10000/10000: episode: 20, duration: 12.018s, episode steps: 500, steps per second: 42, episode reward: -738.886, mean reward: -1.478 [-3.140, -0.677], mean action: 0.233 [-0.567, 1.322], mean observation: -0.518 [-30.826, 209.721], loss: 0.341860, mean_absolute_error: 0.563157, mean_q: -14.953930\n",
      "done, took 279.655 seconds\n"
     ]
    }
   ],
   "source": [
    "nallsteps=10000\n",
    "#Warning: verbose=1 freezes the notebook\n",
    "agent.fit(env, nb_steps=nallsteps, visualize=False, verbose=2, nb_max_episode_steps=env.timestep_limit, log_interval=10000)\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights(args.model, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: -598.730, steps: 500\n",
      "Episode 2: reward: -728.133, steps: 500\n",
      "Episode 3: reward: -727.629, steps: 500\n",
      "Episode 4: reward: -770.379, steps: 500\n",
      "Episode 5: reward: -396.460, steps: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8985341790>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load_weights(args.model)\n",
    "# Finally, evaluate our algorithm for 1 episode.\n",
    "agent.test(env, nb_episodes=5, visualize=False, nb_max_episode_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'critic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c96eaf588f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#evaluate performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mloss_and_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'critic' is not defined"
     ]
    }
   ],
   "source": [
    "#evaluate performance\n",
    "loss_and_metrics = critic.evaluate(x_test, y_test, batch_size=128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opensim-rl)",
   "language": "python",
   "name": "opensim-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
