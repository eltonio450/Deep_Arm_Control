{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of importation\n"
     ]
    }
   ],
   "source": [
    "# Derived from keras-rl\n",
    "import opensim as osim\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Input, merge\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.agents import DDPGAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.random import OrnsteinUhlenbeckProcess\n",
    "\n",
    "from osim.env import *\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "print(\"End of importation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments...\n",
      "Namespace(model='example.h5f', steps=10000, visualize=False)\n",
      "Environment generated\n"
     ]
    }
   ],
   "source": [
    "# Command line parameters\n",
    "#parser = argparse.ArgumentParser(description='Train or test neural net motor controller')\n",
    "#parser.add_argument('--train', dest='train', action='store_true', default=True)\n",
    "#parser.add_argument('--test', dest='train', action='store_false', default=True)\n",
    "#parser.add_argument('--steps', dest='steps', action='store', default=10000)\n",
    "#parser.add_argument('--visualize', dest='visualize', action='store_true', default=False)\n",
    "#parser.add_argument('--model', dest='model', action='store', default=\"example.h5f\")\n",
    "#parser.add_argument('-f', dest='kernel', action='store', default=\"e\")\n",
    "#args = parser.parse_args()\n",
    "\n",
    "print(\"Arguments...\")\n",
    "#print(args)\n",
    "\n",
    "args = argparse.Namespace(model='example.h5f', steps=10000, visualize=False)\n",
    "print(args)\n",
    "# Load walking environment\n",
    "env = ArmEnv(args.visualize)\n",
    "print(\"Environment generated\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Param Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ArmEnv instance>\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(env)\n",
    "print(env.timestep_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "flatten_7 (Flatten)              (None, 14)            0           flatten_input_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_25 (Dense)                 (None, 32)            480         flatten_7[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_25 (Activation)       (None, 32)            0           dense_25[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_26 (Dense)                 (None, 32)            1056        activation_25[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_26 (Activation)       (None, 32)            0           dense_26[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_27 (Dense)                 (None, 32)            1056        activation_26[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_27 (Activation)       (None, 32)            0           dense_27[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_28 (Dense)                 (None, 6)             198         activation_27[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_28 (Activation)       (None, 6)             0           dense_28[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 2790\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "observation_input (InputLayer)   (None, 1, 14)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "action_input (InputLayer)        (None, 6)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 14)            0           observation_input[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "merge_4 (Merge)                  (None, 20)            0           action_input[0][0]               \n",
      "                                                                   flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_29 (Dense)                 (None, 64)            1344        merge_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_29 (Activation)       (None, 64)            0           dense_29[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_30 (Dense)                 (None, 64)            4160        activation_29[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_30 (Activation)       (None, 64)            0           dense_30[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_31 (Dense)                 (None, 64)            4160        activation_30[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_31 (Activation)       (None, 64)            0           dense_31[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_32 (Dense)                 (None, 1)             65          activation_31[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_32 (Activation)       (None, 1)             0           dense_32[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 9729\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "nb_actions = env.action_space.shape[0]\n",
    "\n",
    "# Total number of steps in training\n",
    "nallsteps = args.steps\n",
    "\n",
    "# Create networks for DDPG\n",
    "# Next, we build a very simple model.\n",
    "actor = Sequential()\n",
    "actor.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(32))\n",
    "actor.add(Activation('relu'))\n",
    "actor.add(Dense(nb_actions))\n",
    "actor.add(Activation('sigmoid'))\n",
    "print(actor.summary())\n",
    "\n",
    "action_input = Input(shape=(nb_actions,), name='action_input')\n",
    "observation_input = Input(shape=(1,) + env.observation_space.shape, name='observation_input')\n",
    "flattened_observation = Flatten()(observation_input)\n",
    "x = merge([action_input, flattened_observation], mode='concat')\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(64)(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dense(1)(x)\n",
    "x = Activation('linear')(x)\n",
    "critic = Model(input=[action_input, observation_input], output=x)\n",
    "print(critic.summary())\n",
    "\n",
    "# Set up the agent for training\n",
    "#warning: a too high limit gives errors\n",
    "memory = SequentialMemory(limit=100000, window_length=1)\n",
    "random_process = OrnsteinUhlenbeckProcess(theta=.15, mu=0., sigma=.2, size=env.noutput)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic, critic_action_input=action_input,\n",
    "                  memory=memory, nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                  random_process=random_process, gamma=.99, target_model_update=1e-3,\n",
    "                  delta_clip=1.)\n",
    "#agent = ContinuousDQNAgent(nb_actions=env.noutput, V_model=V_model, L_model=L_model, mu_model=mu_model,\n",
    "#                            memory=memory, nb_steps_warmup=1000, random_process=random_process,\n",
    "#                            gamma=.99, target_model_update=0.1)\n",
    "agent.compile(Adam(lr=.001, clipnorm=1.), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this\n",
    "# slows down training quite a lot. You can always safely abort the training prematurely using\n",
    "# Ctrl + C.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "  500/10000: episode: 1, duration: 8.268s, episode steps: 500, steps per second: 60, episode reward: -1129.567, mean reward: -2.259 [-3.415, -0.315], mean action: 0.360 [-0.256, 1.604], mean observation: -0.368 [-36.067, 151.635], loss: 0.073267, mean_absolute_error: 0.206061, mean_q: -2.254587\n",
      " 1000/10000: episode: 2, duration: 11.338s, episode steps: 500, steps per second: 44, episode reward: -216.021, mean reward: -0.432 [-1.806, -0.017], mean action: 0.315 [-0.535, 1.448], mean observation: 0.971 [-1000.000, 1000.000], loss: 1.908218, mean_absolute_error: 2.083755, mean_q: -3.617379\n",
      " 1500/10000: episode: 3, duration: 14.984s, episode steps: 500, steps per second: 33, episode reward: -406.383, mean reward: -0.813 [-2.031, -0.071], mean action: 0.384 [-0.737, 1.575], mean observation: 3.036 [-1000.000, 1000.000], loss: 1.274501, mean_absolute_error: 1.453171, mean_q: -3.369452\n",
      " 2000/10000: episode: 4, duration: 17.446s, episode steps: 500, steps per second: 29, episode reward: -744.265, mean reward: -1.489 [-3.138, -0.402], mean action: 0.378 [-0.440, 1.970], mean observation: 0.113 [-1000.000, 1000.000], loss: 1.425659, mean_absolute_error: 1.613091, mean_q: -4.344694\n",
      " 2500/10000: episode: 5, duration: 11.983s, episode steps: 500, steps per second: 42, episode reward: -201.611, mean reward: -0.403 [-1.694, -0.011], mean action: 0.356 [-0.646, 1.444], mean observation: -0.129 [-1000.000, 1000.000], loss: 1.086564, mean_absolute_error: 1.249489, mean_q: -4.105164\n",
      " 3000/10000: episode: 6, duration: 10.732s, episode steps: 500, steps per second: 47, episode reward: -658.481, mean reward: -1.317 [-2.728, -0.166], mean action: 0.427 [-0.789, 1.567], mean observation: -0.256 [-1000.000, 1000.000], loss: 1.041212, mean_absolute_error: 1.204871, mean_q: -4.111361\n",
      " 3500/10000: episode: 7, duration: 9.173s, episode steps: 500, steps per second: 55, episode reward: -510.441, mean reward: -1.021 [-1.913, -0.219], mean action: 0.205 [-0.584, 1.441], mean observation: -0.060 [-352.541, 825.277], loss: 0.747911, mean_absolute_error: 0.906899, mean_q: -4.067877\n",
      " 4000/10000: episode: 8, duration: 6.958s, episode steps: 500, steps per second: 72, episode reward: -817.990, mean reward: -1.636 [-2.680, -0.128], mean action: 0.462 [-0.691, 1.669], mean observation: -0.022 [-101.053, 878.573], loss: 0.578638, mean_absolute_error: 0.733100, mean_q: -4.376172\n",
      " 4500/10000: episode: 9, duration: 8.446s, episode steps: 500, steps per second: 59, episode reward: -1075.386, mean reward: -2.151 [-3.952, -0.371], mean action: 0.416 [-0.800, 1.354], mean observation: -0.236 [-18.237, 197.935], loss: 0.398006, mean_absolute_error: 0.556865, mean_q: -4.824623\n",
      " 5000/10000: episode: 10, duration: 8.533s, episode steps: 500, steps per second: 59, episode reward: -498.489, mean reward: -0.997 [-1.325, -0.245], mean action: 0.170 [-0.883, 1.218], mean observation: -0.060 [-19.175, 11.251], loss: 0.307488, mean_absolute_error: 0.460226, mean_q: -5.495394\n",
      " 5500/10000: episode: 11, duration: 11.286s, episode steps: 500, steps per second: 44, episode reward: -272.920, mean reward: -0.546 [-2.113, -0.058], mean action: 0.387 [-0.744, 1.387], mean observation: -0.030 [-21.111, 11.040], loss: 0.221533, mean_absolute_error: 0.366489, mean_q: -5.645409\n",
      " 6000/10000: episode: 12, duration: 13.127s, episode steps: 500, steps per second: 38, episode reward: -217.628, mean reward: -0.435 [-1.929, -0.092], mean action: 0.432 [-0.599, 1.496], mean observation: 2.722 [-1000.000, 1000.000], loss: 0.234938, mean_absolute_error: 0.377489, mean_q: -5.796844\n",
      " 6500/10000: episode: 13, duration: 13.357s, episode steps: 500, steps per second: 37, episode reward: -1114.210, mean reward: -2.228 [-3.298, -0.127], mean action: 0.715 [-0.529, 1.785], mean observation: 1.971 [-1000.000, 1000.000], loss: 0.569478, mean_absolute_error: 0.737801, mean_q: -6.244762\n",
      " 7000/10000: episode: 14, duration: 12.860s, episode steps: 500, steps per second: 39, episode reward: -388.531, mean reward: -0.777 [-1.966, -0.030], mean action: 0.252 [-0.756, 1.351], mean observation: -0.071 [-1000.000, 804.632], loss: 0.662922, mean_absolute_error: 0.841270, mean_q: -6.651261\n",
      " 7500/10000: episode: 15, duration: 10.540s, episode steps: 500, steps per second: 47, episode reward: -432.434, mean reward: -0.865 [-1.412, -0.080], mean action: 0.507 [-0.448, 1.704], mean observation: -0.063 [-77.605, 267.747], loss: 0.494916, mean_absolute_error: 0.681779, mean_q: -6.699060\n",
      " 8000/10000: episode: 16, duration: 9.488s, episode steps: 500, steps per second: 53, episode reward: -247.913, mean reward: -0.496 [-2.063, -0.012], mean action: 0.459 [-0.597, 1.495], mean observation: -0.114 [-18.396, 100.697], loss: 0.418286, mean_absolute_error: 0.603917, mean_q: -6.963116\n",
      " 8500/10000: episode: 17, duration: 9.086s, episode steps: 500, steps per second: 55, episode reward: -222.550, mean reward: -0.445 [-2.510, -0.047], mean action: 0.437 [-0.702, 1.766], mean observation: -0.205 [-1000.000, 766.167], loss: 0.325621, mean_absolute_error: 0.509639, mean_q: -6.869691\n",
      " 9000/10000: episode: 18, duration: 8.439s, episode steps: 500, steps per second: 59, episode reward: -268.215, mean reward: -0.536 [-1.417, -0.189], mean action: 0.204 [-0.579, 1.157], mean observation: -0.090 [-1000.000, 1000.000], loss: 0.304995, mean_absolute_error: 0.499747, mean_q: -7.004058\n",
      " 9500/10000: episode: 19, duration: 10.388s, episode steps: 500, steps per second: 48, episode reward: -1662.057, mean reward: -3.324 [-4.198, -0.257], mean action: 0.440 [-1.033, 1.456], mean observation: 0.133 [-1000.000, 843.241], loss: 0.265355, mean_absolute_error: 0.455238, mean_q: -7.466813\n",
      " 10000/10000: episode: 20, duration: 8.082s, episode steps: 500, steps per second: 62, episode reward: -415.788, mean reward: -0.832 [-1.652, -0.295], mean action: 0.350 [-0.298, 1.356], mean observation: -0.107 [-18.374, 266.349], loss: 0.270670, mean_absolute_error: 0.460878, mean_q: -8.102181\n",
      "done, took 214.575 seconds\n"
     ]
    }
   ],
   "source": [
    "#Warning: verbose=1 freezes the notebook\n",
    "agent.fit(env, nb_steps=nallsteps, visualize=False, verbose=2, nb_max_episode_steps=env.timestep_limit, log_interval=10000)\n",
    "# After training is done, we save the final weights.\n",
    "agent.save_weights(args.model, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: -159.028, steps: 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2da4135b90>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.load_weights(args.model)\n",
    "    # Finally, evaluate our algorithm for 1 episode.\n",
    "agent.test(env, nb_episodes=1, visualize=False, nb_max_episode_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (opensim-rl)",
   "language": "python",
   "name": "opensim-rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
